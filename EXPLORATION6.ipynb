{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ahead-class",
   "metadata": {},
   "source": [
    "# ë©‹ì§„ ì¸ê³µì§€ëŠ¥ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-oregon",
   "metadata": {},
   "source": [
    "ì‘ê³¡ì–¸ì–´ëŠ” (ì•„ì‰½ê²Œë„)ì˜ì–´ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ë§Œë“¤ì–´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-melbourne",
   "metadata": {},
   "source": [
    "> **Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ  \n",
    "Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°  \n",
    "Step 3. ë°ì´í„° ì •ì œ  \n",
    "Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬  \n",
    "Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-spice",
   "metadata": {},
   "source": [
    "ê·¸ëŸ¼ ì‹œì‘í•´ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-earthquake",
   "metadata": {},
   "source": [
    "### Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-friendship",
   "metadata": {},
   "source": [
    "> ë¨¼ì € ì•„ë˜ ë§í¬ì—ì„œ Song Lyrics ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”!<br>ì €ì¥ëœ íŒŒì¼ì„ ì••ì¶• í•´ì œí•œ í›„, ëª¨ë“  txt íŒŒì¼ì„ lyrics í´ë”ë¥¼ ë§Œë“¤ì–´ ê·¸ ì†ì— ì €ì¥í•´ì£¼ì„¸ìš”!<br>ì•„ë‹ˆë©´ ê·¸ ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì…”ë„ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-fiction",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/paultimothymooney/poetry/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "narrow-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "# $ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics í´ë”ì— ì••ì¶•í’€ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-works",
   "metadata": {},
   "source": [
    "### Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-standing",
   "metadata": {},
   "source": [
    "glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë©´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì‘ì—…ì„ í•˜ê¸°ê°€ ì•„ì£¼ ìš©ì´í•´ìš”.  \n",
    "glob ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ë„ë¡ í• ê²Œìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "young-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " ['', '', \"Jesus died for somebody's sins but not mine\"]\n"
     ]
    }
   ],
   "source": [
    "import re                  # ì •ê·œí‘œí˜„ì‹ì„ ìœ„í•œ Regex ì§€ì› ëª¨ë“ˆ (ë¬¸ì¥ ë°ì´í„°ë¥¼ ì •ëˆí•˜ê¸° ìœ„í•´) \n",
    "import numpy as np         # ë³€í™˜ëœ ë¬¸ì¥ ë°ì´í„°(í–‰ë ¬)ì„ í¸í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´\n",
    "import tensorflow as tf    # ëŒ€ë§ì˜ í…ì„œí”Œë¡œìš°!\n",
    "import glob                # íŒŒì¼ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë½‘ì„ ë•Œ ì‚¬ìš©\n",
    "import os                  # ìš´ì˜ ì²´ì œì˜ í™˜ê²½ ë³€ìˆ˜ ê°’ì„ ì½ì–´ì˜¬ ìˆ˜ ìˆìŒ!\n",
    "\n",
    "# íŒŒì¼ì„ ì½ê¸°ëª¨ë“œë¡œ ì—´ì–´ ë´…ë‹ˆë‹¤.\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'  # * : all\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []  # ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpusì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-prague",
   "metadata": {},
   "source": [
    "### Step 3. ë°ì´í„° ì •ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-sussex",
   "metadata": {},
   "source": [
    "##### ì´ë²ˆ ìŠ¤í…ì—ì„œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ê±°ì¹  ê³¼ì •ì…ë‹ˆë‹¤. \n",
    "\n",
    ">- ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ corpus ìƒì„±\n",
    ">- tf.keras.preprocessing.text.Tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    ">- tf.data.Dataset.from_tensor_slices()ë¥¼ ì´ìš©í•´ corpus í…ì„œë¥¼ tf.data.Datasetê°ì²´ë¡œ ë³€í™˜\n",
    "\n",
    "datasetì„ ì–»ìœ¼ë©´ ë°ì´í„° ë‹¤ë“¬ê¸° ê³¼ì •ì€ ëë‚©ë‹ˆë‹¤.<br>\n",
    "tf.data.Datasetì—ì„œ ì œê³µí•˜ëŠ” shuffle(), batch() ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ê´€ë ¨ ê¸°ëŠ¥ì„ ì†ì‰½ê²Œ ì´ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆêµ°ìš”.\n",
    "\n",
    "ì´ ëª¨ë“  ì¼ë ¨ì˜ ê³¼ì •ì„ í…ì„œí”Œë¡œìš°ì—ì„œì˜ **ë°ì´í„° ì „ì²˜ë¦¬**ë¼ ì¹­í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-carol",
   "metadata": {},
   "source": [
    "**ë¬¸ì¥ ìƒì„±ì— ì í•©í•œ ëª¨ì–‘ìƒˆë¡œ ë°ì´í„°ë¥¼ ì •ì œí•˜ì„¸ìš”!**\n",
    "\n",
    "preprocess_sentence() í•¨ìˆ˜ë¥¼ ë§Œë“  ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ? ì´ë¥¼ í™œìš©í•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ê°€ë¡œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ê³¼ë„í•œ Paddingì„ ê°–ê²Œ í•˜ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤. ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë…¸ë˜ê°€ì‚¬ ì‘ì‚¬í•˜ê¸°ì— ì–´ìš¸ë¦¬ì§€ ì•Šì„ìˆ˜ë„ ìˆê² ì£ .\n",
    "ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” ë¬¸ì¥ì„ **í† í°í™”** í–ˆì„ ë•Œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì„ í•™ìŠµë°ì´í„°ì—ì„œ ì œì™¸í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-lawrence",
   "metadata": {},
   "source": [
    "ê°€ì¥ ì‹¬í”Œí•œ ë°©ë²•ì€ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ì§€ë§Œ ì´ ë°©ë²•ì—ëŠ” ëª‡ê°€ì§€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "##### ëª‡ ê°€ì§€ ë¬¸ì œ ì¼€ì´ìŠ¤\n",
    ">1. Hi, my name is John. *(\"Hi,\" \"my\", â€¦, \"john.\" ìœ¼ë¡œ ë¶„ë¦¬ë¨) - ë¬¸ì¥ë¶€í˜¸\n",
    ">2. First, open the first chapter. *(Firstì™€ firstë¥¼ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹) - ëŒ€ì†Œë¬¸ì\n",
    ">3. He is a ten-year-old boy. *(ten-year-oldë¥¼ í•œ ë‹¨ì–´ë¡œ ì¸ì‹) - íŠ¹ìˆ˜ë¬¸ì\n",
    "\n",
    "\"1.\" ì„ ë§‰ê¸° ìœ„í•´ ë¬¸ì¥ ë¶€í˜¸ ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€ í•  ê±°ê³ ìš”,<br>\n",
    "\"2.\" ë¥¼ ë§‰ê¸° ìœ„í•´ ëª¨ë“  ë¬¸ìë“¤ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•  ê²ë‹ˆë‹¤.<br>\n",
    "\"3.\" ì„ ë§‰ê¸° ìœ„í•´ íŠ¹ìˆ˜ë¬¸ìë“¤ì€ ëª¨ë‘ ì œê±°í•˜ë„ë¡ í•˜ì£ !\n",
    "\n",
    "ì´ëŸ° ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ì •ê·œí‘œí˜„ì‹(Regex)ì„ ì´ìš©í•œ í•„í„°ë§ì´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "animal-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ì–‘ìª½ ê³µë°±ì„ ì‚­ì œ\n",
    "  \n",
    "    # ì•„ë˜ 3ë‹¨ê³„ë¥¼ ê±°ì³ sentenceëŠ” ìŠ¤í˜ì´ìŠ¤ 1ê°œë¥¼ delimeterë¡œ í•˜ëŠ” ì†Œë¬¸ì ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë°”ë€ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)        # íŒ¨í„´ì˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë§Œë‚˜ë©´ íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # ê³µë°± íŒ¨í„´ì„ ë§Œë‚˜ë©´ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence)  # a-zA-Z?.!,Â¿ íŒ¨í„´ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ê³µë°±ë¬¸ìê¹Œì§€ë„)ë¥¼ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # ì´ì „ ìŠ¤í…ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ ë¬¸ì¥ ì•ë’¤ë¡œ <start>ì™€ <end>ë¥¼ ë‹¨ì–´ì²˜ëŸ¼ ë¶™ì—¬ ì¤ë‹ˆë‹¤\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-equality",
   "metadata": {},
   "source": [
    "ì§œì”, ì§€ì €ë¶„í•œ ë¬¸ì¥ì„ ë„£ì–´ë„ ì˜ˆì˜ê²Œ ë³€í™˜í•´ì£¼ëŠ” ì •ì œ í•¨ìˆ˜ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!<br>ë³´ë„ˆìŠ¤ë¡œ start, end ë„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ëª¨ë¸ì˜ ì…ë ¥ì´ ë˜ëŠ” ë¬¸ì¥ì„ ì†ŒìŠ¤ ë¬¸ì¥(Source Sentence), ì •ë‹µ ì—­í• ì„ í•˜ê²Œ ë  ëª¨ë¸ì˜ ì¶œë ¥ ë¬¸ì¥ì„ íƒ€ê²Ÿ ë¬¸ì¥(Target Sentence)ë¼ê³  ê´€ë¡€ì ìœ¼ë¡œ ë¶€ë¦…ë‹ˆë‹¤. ê°ê° X_train, y_train ì— í•´ë‹¹í•œë‹¤ê³  í•  ìˆ˜ ìˆê² ì£ ?\n",
    "\n",
    "ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ìœ„ì—ì„œ ë§Œë“  ì •ì œ í•¨ìˆ˜ë¥¼ í†µí•´ ë§Œë“  ë°ì´í„°ì…‹ì—ì„œ í† í°í™”ë¥¼ ì§„í–‰í•œ í›„ ë ë‹¨ì–´ <end>ë¥¼ ì—†ì• ë©´ ì†ŒìŠ¤ ë¬¸ì¥, ì²« ë‹¨ì–´ <start>ë¥¼ ì—†ì• ë©´ íƒ€ê²Ÿ ë¬¸ì¥ì´ ë˜ê² ì£ ? ì´ ì •ì œ í•¨ìˆ˜ë¥¼ í™œìš©í•´ì„œ ì•„ë˜ì™€ ê°™ì´ ì •ì œ ë°ì´í„°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sticky-technician",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> jesus died for somebody s sins but not mine <end>',\n",
       " '<start> meltin in a pot of thieves <end>',\n",
       " '<start> wild card up my sleeve <end>',\n",
       " '<start> thick heart of stone <end>',\n",
       " '<start> my sins my own <end>',\n",
       " '<start> they belong to me , me <end>',\n",
       " '<start> people say beware ! <end>',\n",
       " '<start> but i don t care <end>',\n",
       " '<start> the words are just <end>',\n",
       " '<start> rules and regulations to me , me <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-signature",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„°ëŠ” ì™„ë²½í•˜ê²Œ ì¤€ë¹„ê°€ ëœ ê²ƒ ê°™ë„¤ìš”!<br>\n",
    "\n",
    "í…ì„œí”Œë¡œìš°ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì—¬ëŸ¬ ê°€ì§€ ëª¨ë“ˆì„ ì œê³µí•˜ëŠ”ë°, ìš°ë¦¬ë„ ê·¸ ëª¨ë“ˆì„ ì‹­ë¶„ í™œìš©í•  ê²ë‹ˆë‹¤!<br>\n",
    "ì•„ë˜ì—ì„œ í™œìš©í•˜ê²Œ ë  **tf.keras.preprocessing.text.Tokenizer íŒ¨í‚¤ì§€**ëŠ” ì •ì œëœ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ë‹¨ì–´ ì‚¬ì „(vocabulary ë˜ëŠ” dictionaryë¼ê³  ì¹­í•¨)ì„ ë§Œë“¤ì–´ì£¼ë©°, ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜ê¹Œì§€ í•œ ë°©ì— í•´ì¤ë‹ˆë‹¤.<br>\n",
    "ì´ ê³¼ì •ì„ ë²¡í„°í™”(vectorize) ë¼ í•˜ë©°, ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ í…ì„œ(tensor) ë¼ê³  ì¹­í•©ë‹ˆë‹¤.<br>\n",
    "ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” í…ì„œí”Œë¡œìš°ë¡œ ë§Œë“  ëª¨ë¸ì˜ ì…ì¶œë ¥ ë°ì´í„°ëŠ” ì‹¤ì œë¡œëŠ” ëª¨ë‘ ì´ëŸ° í…ì„œë¡œ ë³€í™˜ë˜ì–´ ì²˜ë¦¬ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collectible-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  641  732 ...    0    0    0]\n",
      " [   2    1   14 ...    0    0    0]\n",
      " [   2  492 2040 ...    0    0    0]\n",
      " ...\n",
      " [   2 4417    6 ...    0    0    0]\n",
      " [   2   89   15 ...    0    0    0]\n",
      " [   2  109   78 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f73eb46cb50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # í…ì„œí”Œë¡œìš°ì—ì„œ ì œê³µí•˜ëŠ” Tokenizer íŒ¨í‚¤ì§€ë¥¼ ìƒì„±\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜ \n",
    "        filters=' ',    # ë³„ë„ë¡œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, ì‚¬ì „ì— ì—†ì—ˆë˜ ë‹¨ì–´ëŠ” ì–´ë–¤ í† í°ìœ¼ë¡œ ëŒ€ì²´í• ì§€\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # ìš°ë¦¬ê°€ êµ¬ì¶•í•œ corpusë¡œë¶€í„° Tokenizerê°€ ì‚¬ì „ì„ ìë™êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "    # ì´í›„ tokenizerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizerëŠ” êµ¬ì¶•í•œ ì‚¬ì „ìœ¼ë¡œë¶€í„° corpusë¥¼ í•´ì„í•´ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•œ padding  ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "    # maxlenì˜ ë””í´íŠ¸ê°’ì€ Noneì…ë‹ˆë‹¤. ì´ ê²½ìš° corpusì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë§ì¶°ì§‘ë‹ˆë‹¤.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-durham",
   "metadata": {},
   "source": [
    "í…ì„œ ë°ì´í„°ëŠ” ëª¨ë‘ ì •ìˆ˜ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.<br>\n",
    "ì´ ìˆ«ìëŠ” ë‹¤ë¦„ ì•„ë‹ˆë¼, tokenizerì— êµ¬ì¶•ëœ ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.<br>\n",
    "ë‹¨ì–´ ì‚¬ì „ì´ ì–´ë–»ê²Œ êµ¬ì¶•ë˜ì—ˆëŠ”ì§€ ì•„ë˜ì™€ ê°™ì´ í™•ì¸í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alien-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-gibraltar",
   "metadata": {},
   "source": [
    "ì–´ë–»ìŠµë‹ˆê¹Œ? 2ë²ˆ ì¸ë±ìŠ¤ê°€ ë°”ë¡œ **start**ì˜€ìŠµë‹ˆë‹¤.<br>\n",
    "ì™œ ëª¨ë“  í–‰ì´ 2ë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ìƒì„±ëœ í…ì„œë¥¼ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ê² ìŠµë‹ˆë‹¤.<br>\n",
    "ì´ ê³¼ì •ë„í…ì„œí”Œë¡œìš°ê°€ ì œê³µí•˜ëŠ” ëª¨ë“ˆì„ ì‚¬ìš©í•  ê²ƒì´ë‹ˆ, ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ë§Œ ëˆˆì—¬ê²¨ ë´ë‘¡ì‹œë‹¤.\n",
    "\n",
    "í…ì„œ ì¶œë ¥ë¶€ì—ì„œ í–‰ ë’¤ìª½ì— 0ì´ ë§ì´ ë‚˜ì˜¨ ë¶€ë¶„ì€ ì •í•´ì§„ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë³´ë‹¤ ë¬¸ì¥ì´ ì§§ì„ ê²½ìš° 0ìœ¼ë¡œ íŒ¨ë”©(padding)ì„ ì±„ì›Œë„£ì€ ê²ƒì…ë‹ˆë‹¤.<br>\n",
    "ì‚¬ì „ì—ëŠ” ì—†ì§€ë§Œ 0ì€ ë°”ë¡œ íŒ¨ë”© ë¬¸ì **pad**ê°€ ë  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exclusive-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2  641  732   28  265   16 2396   36   70  216    3    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n",
      "[ 641  732   28  265   16 2396   36   70  216    3    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    # tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-hands",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ìƒì„±í•  ê²ƒì…ë‹ˆë‹¤.<br>\n",
    "í…ì„œí”Œë¡œìš°ë¥¼ í™œìš©í•  ê²½ìš° í…ì„œë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì´ìš©í•´ tf.data.Datasetê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í”íˆ ì‚¬ìš©í•©ë‹ˆë‹¤.<br>\n",
    "tf.data.Datasetê°ì²´ëŠ” í…ì„œí”Œë¡œìš°ì—ì„œ ì‚¬ìš©í•  ê²½ìš° ë°ì´í„° ì…ë ¥ íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ì†ë„ ê°œì„  ë° ê°ì¢… í¸ì˜ê¸°ëŠ¥ì„ ì œê³µí•˜ë¯€ë¡œ ê¼­ ì‚¬ìš©ë²•ì„ ì•Œì•„ ë‘ëŠ”ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.<br>\n",
    "ì´ë¯¸ ìœ„ì—ì„œ ë°ì´í„°ì…‹ì„ í…ì„œ í˜•íƒœë¡œ ìƒì„±í•´ ë‘ì—ˆìœ¼ë¯€ë¡œ, tf.data.Dataset.from_tensor_slices() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ tf.data.Datasetê°ì²´ë¥¼ ìƒì„±í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "higher-volleyball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 346), (256, 346)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 7000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-writer",
   "metadata": {},
   "source": [
    "### Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-estonia",
   "metadata": {},
   "source": [
    "**í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì„¸ìš”!**\n",
    "\n",
    "tokenize() í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•œ í›„, sklearn ëª¨ë“ˆì˜ train_test_split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  \n",
    "ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”!  \n",
    "\n",
    "ì´ ë°ì´í„°ì˜ **20%** ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "received-collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 346)\n",
      "Target Train: (140599, 346)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-square",
   "metadata": {},
   "source": [
    "ë§Œì•½ í•™ìŠµë°ì´í„° ê°¯ìˆ˜ê°€ 124960ë³´ë‹¤ í¬ë‹¤ë©´ ìœ„ Step 3.ì˜ ë°ì´í„° ì •ì œ ê³¼ì •ì„ ë‹¤ì‹œí•œë²ˆ ê²€í† í•´ ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-membrane",
   "metadata": {},
   "source": [
    "Source Train: (140599, 346)<br>\n",
    "Target Train: (140599, 346)<br>\n",
    "ì´ ë‚˜ì™”ë‹¤. ì–´ë–»ê²Œ ì¤„ì—¬ì•¼ í• ê¹Œ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-cathedral",
   "metadata": {},
   "source": [
    "### Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-mediterranean",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì˜ **Embedding Size**ì™€ **Hidden Size**ë¥¼ ì¡°ì ˆí•˜ë©° **10 Epoch** ì•ˆì— **val_loss ê°’ì„ 2.2 ìˆ˜ì¤€**ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ì„¸ìš”!  \n",
    "(LossëŠ” ì•„ë˜ ì œì‹œëœ Loss í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "norman-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss í•¨ìˆ˜\n",
    "\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-graphic",
   "metadata": {},
   "source": [
    "\"\"\"\"\"\", '''''' ë¡œ ì¤„ ì£¼ì„ì²˜ë¦¬ í•˜ë ¤ë‹ˆ ì•ˆë˜ë„¤ìš”..  \n",
    "ì£¼ì„ì„ í•˜ê³  ì‹¶ì€ ë¶€ë¶„ë“¤ì„ ë“œë˜ê·¸ í•´ì„œ ì˜ì—­ì„ ì„ íƒí•œ í›„ Ctrl + / ë¥¼ ëˆ„ë¥´ë©´ ëœë‹¤ê¸¸ë˜ í•´ë³´ë‹ˆ #ìœ¼ë¡œ ì¤„ë§ˆë‹¤ ì£¼ì„ì²˜ë¦¬ ë©ë‹ˆë‹¤.ğŸ¤”ğŸ¤”ğŸ¤”\n",
    "ì´ ë¶€ë¶„ì€ ì¶”í›„ ë‹¤ì‹œ ì•Œì•„ë³´ê¸°ë¡œ.....ğŸ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "familiar-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-highlight",
   "metadata": {},
   "source": [
    "modelì˜ input shapeê°€ ê²°ì •ë˜ë©´ì„œ model.build()ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-northwest",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì´ í•™ìŠµí•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.<br>\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œë³´ì„¸ìš”!<br>\n",
    "í•™ìŠµì—” 10ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤(GPU í™˜ê²½ ê¸°ì¤€)<br>\n",
    "í˜¹ì‹œë¼ë„ í•™ìŠµì— ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤ë©´ tf.test.is_gpu_available() ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´ í…ì„œí”Œë¡œìš°ê°€ GPUë¥¼ ì˜ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  \n",
    " \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # ë¬¸ì¥ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ ì¼ë‹¨ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í• ë•ŒëŠ” ë£¨í”„ë¥¼ ëŒë©´ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë°”ë¡œ ìƒˆë¡­ê²Œ ìƒì„±í•œ ë‹¨ì–´ê°€ ë©ë‹ˆë‹¤. \n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ ìƒˆë¡­ê²Œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ ë¬¸ì¥ì˜ ë’¤ì— ë¶™ì—¬ ì¤ë‹ˆë‹¤. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´  while ë£¨í”„ë¥¼ ë˜ ëŒë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # ìƒì„±ëœ tensor ì•ˆì— ìˆëŠ” word indexë¥¼ tokenizer.index_word ì‚¬ì „ì„ í†µí•´ ì‹¤ì œ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-festival",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ ìƒì„±í•œ ê°€ì‚¬ í•œ ì¤„ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-section",
   "metadata": {},
   "source": [
    "ë¼ ê·¸ëŸ´ë“¯í•˜ì£ ? ã…ã…ã…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-penguin",
   "metadata": {},
   "source": [
    "## ë£¨ë¸Œë¦­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-museum",
   "metadata": {},
   "source": [
    "1. ê°€ì‚¬ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ê°€?\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´ì…˜ ê²°ê³¼ê°€ ê·¸ëŸ´ë“¯í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±ë˜ëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-hudson",
   "metadata": {},
   "source": [
    "2. ë°ì´í„°ì˜ ì „ì²˜ë¦¬ì™€ ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ê°€?\n",
    "\n",
    "íŠ¹ìˆ˜ë¬¸ì ì œê±°, í† í¬ë‚˜ì´ì € ìƒì„±, íŒ¨ë”©ì²˜ë¦¬ ë“±ì˜ ê³¼ì •ì´ ë¹ ì§ì—†ì´ ì§„í–‰ë˜ì—ˆëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-memphis",
   "metadata": {},
   "source": [
    "3. í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆëŠ”ê°€?\n",
    "\n",
    "í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì˜ validation lossê°€ 2.2 ì´í•˜ë¡œ ë‚®ì•„ì¡ŒëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-berkeley",
   "metadata": {},
   "source": [
    "## íšŒê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-nowhere",
   "metadata": {},
   "source": [
    "ë¹¡ì„¼ ì €ë²ˆì£¼ì˜ ë…¸ë“œë³´ë‹¤ í›¨ì”¬ ê°„ê²°í•´ì„œ ì´ê²Œ ë§ë‚˜...? í–ˆëŠ”ë° ì¡°ì›ë“¤ê³¼ ì–˜ê¸°í•´ë³´ë‹ˆ ì €ë²ˆì£¼ ë…¸ë“œê³¼ì œë“¤ì´ ë‚œë„ê°€ ë†’ì•˜ë‹¤ëŠ” ê±¸ ìƒˆì‚¼ ëŠê¼ˆë‹¤.  \n",
    "ì•„ë¬´ë˜ë„ ğŸ‡°ğŸ‡·í•œêµ­ì–´ë³´ë‹¤ ê³µê°œëœ ğŸ‡ºğŸ‡¸ì˜ì–´ë°ì´í„°ê°€ ë§ì•„ ì•„ì‰½ê¸´í•˜ì§€ë§Œ ì´ë²ˆì£¼ ë…¸ë“œëŠ” ì¢€ ë” ì¬ë°Œê²Œ í•  ìˆ˜ ìˆì—ˆë‹¤.  \n",
    "ë‹¤ë§Œ ì˜ì–´ê³µë¶€ë„ ë‹¤ì‹œ í•´ì•¼...í•  í•„ìš”ì„±ì„ ê³„ì† ëŠë¼ê³  ìˆì–´ì„œ ê³„íšì„ ì„¸ì›Œë´ì•¼ê² ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
