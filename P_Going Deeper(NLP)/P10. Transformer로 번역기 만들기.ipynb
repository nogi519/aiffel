{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-nancy",
   "metadata": {},
   "source": [
    "# <center><span style=\"color:#2C786C\">프로젝트 : 더 멋진 번역기 만들기</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-accused",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-puppy",
   "metadata": {},
   "source": [
    "## <span style=\"color:#F7B400\">순서</span>\n",
    "> **<span style=\"color:#2C786C\">Step 1-1. 데이터 다운로드 (로컬 유저용)</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 1-2. 데이터 다운로드 (클라우드 유저용)</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 2. 데이터 정제 및 토큰화</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 3. 모델 설계</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 4. 훈련하기</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 5. 정리</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 6. 루브릭 평가</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 7. 회고</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-rhythm",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-branch",
   "metadata": {},
   "source": [
    "### <span style=\"color:#926DD6\">참고</span>\n",
    ">* []()\n",
    ">* []()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-retirement",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-professor",
   "metadata": {},
   "source": [
    "### <span style=\"color:#926DD6\">개념</span>\n",
    ">*\n",
    "\n",
    ">*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-induction",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-justice",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 1-1. 데이터 다운로드 (로컬 유저용)</span>\n",
    "---\n",
    "아래 링크에서 korean-english-park.train.tar.gz 를 다운로드받아 한영 병렬 데이터를 확보합니다.\n",
    "\n",
    "* [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-defensive",
   "metadata": {},
   "source": [
    "*💡이전 [ Seq2seq으로 번역기 만들기 ] 코스에서 사용한 데이터와 동일한 데이터입니다!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-spirituality",
   "metadata": {},
   "source": [
    "터미널을 열어서 하단의 명령어를 입력해주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-newark",
   "metadata": {},
   "source": [
    "    $ mkdir -p ~/aiffel/transformer/data\n",
    "    \n",
    "    $ cd ~/aiffel/transformer/data\n",
    "\n",
    "    $ wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
    "    \n",
    "    $ gzip -d korean-english-park.train.tar.gz\n",
    "    \n",
    "    $ tar -xvf korean-english-park.train.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-allah",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-destruction",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 1-2. 데이터 다운로드 (로컬 유저용)</span>\n",
    "---\n",
    "☁️클라우드를 이용중이신 분은 우측하단의 Cloud shell을 열어주세요.\n",
    "아래와 같이 공유디렉토리에 저장된 데이터를 가리키는 심볼릭 링크를 생성해 주시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-lesson",
   "metadata": {},
   "source": [
    "    $ ln -s ~/data ~/aiffel/transformer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-pattern",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-letter",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 2. 추출된 결과로 embedding model 만들기</span>\n",
    "---\n",
    "1) **`set`** 데이터형이 **중복을 허용하지 않는다는 것을 활용**해 중복된 데이터를 제거하도록 합니다. 데이터의 **병렬 쌍이 흐트러지지 않게 주의**하세요! 중복을 제거한 데이터를 **`cleaned_corpus`**에 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-groove",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "second-accident",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-837e0548f5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HOME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/aiffel/transformer/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkor_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/korean-english-park.train.ko\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meng_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/korean-english-park.train.en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 데이터 정제 및 토큰화\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-interface",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-dance",
   "metadata": {},
   "source": [
    "2) 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
    "> 조건\n",
    ">* *모든 입력을 **소문자로 변환**합니다.*\n",
    ">* ***알파벳, 문장부호, 한글**만 남기고 모두 제거합니다.*\n",
    ">* ***문장부호 양옆에 공백**을 추가합니다.*\n",
    ">* *문장 앞뒤의 **불필요한 공백을 제거**합니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-reggae",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-thousand",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-action",
   "metadata": {},
   "source": [
    "2) 정제 함수를 아래 조건을 만족하게 정의하세요.\n",
    "> 조건\n",
    ">* *모든 입력을 **소문자로 변환**합니다.*\n",
    ">* ***알파벳, 문장부호, 한글*** *만 남기고 모두 제거합니다.*\n",
    ">* ***문장부호 양옆에 공백*** *을 추가합니다.*\n",
    ">* *문장 앞뒤의 **불필요한 공백을 제거**합니다.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-royal",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-memphis",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-sector",
   "metadata": {},
   "source": [
    "3) 한글 말뭉치 **`kor_corpus`**와 영문 말뭉치 **`eng_corpus`**를 각각 분리한 후, **정제하여 토큰화를 진행**합니다! **토큰화에는 *Sentencepiece*를 활용**하세요. 첨부된 공식 사이트를 참고해 아래 조건을 만족하는 **`generate_tokenizer()`** 함수를 정의합니다. 최종적으로 **`ko_tokenizer`** 과 **`en_tokenizer`** 를 얻으세요. **`en_tokenizer`**에는 **`set_encode_extra_options(\"bos:eos\")`** 함수를 실행해 **타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게** 합니다.\n",
    "> 조건\n",
    ">* *학습 후 저장된 **`model`** 파일을 **`SentencePieceProcessor()`** 클래스에 **`Load()`**한 후 반환합니다.*\n",
    ">* ***특수 토큰의 인덱스*** *를 아래와 동일하게 지정합니다.<br>**`<PAD>`** : 0 / **`<BOS>`** : 1 / **`<EOS>`** : 2 / **`<UNK>`** : 3*\n",
    "\n",
    "\n",
    "* 참고: [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-objective",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-brush",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-powder",
   "metadata": {},
   "source": [
    "4) 토크나이저를 활용해 **토큰의 길이가 50 이하**인 데이터를 선별하여 **`src_corpus`** 와 **`tgt_corpus`** 를 각각 구축하고, 텐서 **`enc_train`** 과 **`dec_train`** 으로 변환하세요! (❗모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-while",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook    # Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-signature",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-leone",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 3. 모델 설계</span>\n",
    "---\n",
    "오늘 배운 내용을 활용해서 **`Transformer`** 모델을 설계해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-bermuda",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-calcium",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 4. 훈련하기</span>\n",
    "---\n",
    "앞서 필요한 것들을 모두 정의했기 때문에 우리는 훈련만 하면 됩니다! 아래 과정을 차근차근 따라가며 모델을 훈련하고, **예문에 대한 멋진 번역**을 제출하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-drive",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-bibliography",
   "metadata": {},
   "source": [
    "1. **2 Layer**를 가지는 **`Transformer`**를 선언하세요.\n",
    "(하이퍼파라미터는 자유롭게 조절합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-ambassador",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-queue",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-stroke",
   "metadata": {},
   "source": [
    "2. 논문에서 사용한 것과 동일한 **Learning Rate Scheduler**를 선언하고, 이를 포함하는 **Adam Optimizer**를 선언하세요. (Optimizer의 파라미터 역시 논문과 동일하게 설정합니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-celebrity",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = # [[YOUR CODE]]\n",
    "optimizer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-capability",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-interaction",
   "metadata": {},
   "source": [
    "3. **Loss 함수를 정의**하세요.\n",
    "Sequence-to-sequence 모델에서 사용했던 Loss와 유사하되, **Masking 되지 않은 입력의 개수로 Scaling**하는 과정을 추가합니다. (트랜스포머가 모든 입력에 대한 Loss를 한 번에 구하기 때문입니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-senate",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-condition",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-elevation",
   "metadata": {},
   "source": [
    "4. **`train_step`** 함수를 정의하세요. **입력 데이터에 알맞은 Mask를 생성**하고, 이를 모델에 전달하여 연산에서 사용할 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-visit",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-conditions",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-practitioner",
   "metadata": {},
   "source": [
    "5. **학습을 진행**합니다. **매 Epoch 마다 제시된 예문에 대한 번역을 생성**하고, 멋진 번역이 생성되면 그때의 **하이퍼파라미터와 생성된 번역을 제출**하세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-thing",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-apartment",
   "metadata": {},
   "source": [
    "예문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-track",
   "metadata": {},
   "source": [
    "    1. 오바마는 대통령이다.\n",
    "    2. 시민들은 도시 속에 산다.\n",
    "    3. 커피는 필요 없다.\n",
    "    4. 일곱 명의 사망자가 발생했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-minister",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-socket",
   "metadata": {},
   "source": [
    "결과(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-metadata",
   "metadata": {},
   "source": [
    "    Translations\n",
    "    > 1. obama is the president elect .\n",
    "    > 2. they are in the city .\n",
    "    > 3. they don t need to be a lot of drink .\n",
    "    > 4. seven other people have been killed in the attacks .\n",
    "\n",
    "    Hyperparameters\n",
    "    > n_layers: 2\n",
    "    > d_model: 512\n",
    "    > n_heads: 8\n",
    "    > d_ff: 2048\n",
    "    > dropout: 0.3\n",
    "\n",
    "    Training Parameters\n",
    "    > Warmup Steps: 4000\n",
    "    > Batch Size: 64\n",
    "    > Epoch At: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-permit",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-assets",
   "metadata": {},
   "source": [
    "번역 생성에는 아래 소스를 사용하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-wilderness",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-hunter",
   "metadata": {},
   "source": [
    "**`translate()`** 함수의 **`plot_attention`** 변수를 **`True`** 로 주면 **번역 결과에 대한 Attention Map을 시각화** 해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-baltimore",
   "metadata": {},
   "source": [
    "💡 이번 프로젝트에서 제시한 예문은 **Seq2seq으로 번역기 만들기**의 예문과 동일합니다. Seq2seq과 Transformer로 만든 두 번역기의 성능을 하이퍼파라미터를 조정 등 다양한 연구해보시면 학습에 도움이 되실 거예요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-powder",
   "metadata": {},
   "source": [
    "마지막으로, 학습의 전 과정을 구현한 코드를 첨부합니다. 구현과정에 참고해 주세요. 수고하셨습니다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-virus",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-northwest",
   "metadata": {},
   "source": [
    "    # 학습 과정 예시\n",
    "\n",
    "    from tqdm import tqdm_notebook \n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 20\n",
    "\n",
    "    examples = [\n",
    "                \"오바마는 대통령이다.\",\n",
    "                \"시민들은 도시 속에 산다.\",\n",
    "                \"커피는 필요 없다.\",\n",
    "                \"일곱 명의 사망자가 발생했다.\"\n",
    "    ]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm_notebook(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                        dec_train[idx:idx+BATCH_SIZE],\n",
    "                        transformer,\n",
    "                        optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "        for example in examples:\n",
    "            translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-wyoming",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-lodge",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 5. 정리</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-tampa",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-birmingham",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 6. 루브릭 평가</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-concentrate",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|-------|--------|\n",
    "|1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 잘 이루어졌다.|데이터 정제, SentencePiece를 활용한 토큰화 및 데이터셋 구축의 과정이 지시대로 진행되었다.|\n",
    "|2. Transformer 번역기 모델이 정상적으로 구동된다.|Transformer 모델의 학습과 추론 과정이 정상적으로 진행되어, 한-영 번역기능이 정상 동작한다.|\n",
    "|3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.|제시된 문장에 대한 그럴듯한 영어 번역문이 생성되며, 시각화된 Attention Map으로 결과를 뒷받침한다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-trunk",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-bulgarian",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 7. 회고</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-episode",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
