{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "distinguished-trinidad",
   "metadata": {},
   "source": [
    "# <span style=\"color:#2C786C\">2. 프로젝트: SentencePiece 사용하기</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-northern",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-psychology",
   "metadata": {},
   "source": [
    "## <span style=\"color:#F7B400\">순서</span>\n",
    "> **<span style=\"color:#2C786C\">Step 1. SentencePiece 설치하기</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 2. SentencePiece 모델 학습</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 3. Tokenizer 함수 작성</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 4. 네이버 영화리뷰 감정분석 문제에 SentencePiece 적용해 보기</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 5. 루브릭 평가</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 6. 회고</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-fundamentals",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-ecology",
   "metadata": {},
   "source": [
    "## <span style=\"color:#124445\">Step 1. SentencePiece 설치하기</span>\n",
    "---\n",
    "SentencePiece는 Google에서 제공하는 오픈소스 기반 Sentence Tokenizer/Detokenizer 로서, BPE와 unigram 2가지 subword 토크나이징 모델 중 하나를 선택해서 사용할 수 있도록 패키징한 것입니다. 아래 링크의 페이지에서 상세한 내용을 파악할 수 있습니다.\n",
    "\n",
    "* [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-fifteen",
   "metadata": {},
   "source": [
    "위 페이지의 서두에서도 언급하고 있듯, SentencePiece는 딥러닝 자연어처리 모델의 앞부분에 사용할 목적으로 최적화되어 있는데, 최근 pretrained model들이 대부분 SentencePiece를 tokenizer로 채용하면서 사실상 표준의 역할을 하고 있습니다. 앞으로의 실습 과정에서 자주 만나게 될 것이므로 꼭 친숙해지시기를 당부드립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-relation",
   "metadata": {},
   "source": [
    "다음과 같이 설치를 진행합니다. SentencePiece는 python에서 쓰라고 만들어진 라이브러리는 아니지만 편리한 파이썬 wrapper를 아래와 같이 제공하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-chassis",
   "metadata": {},
   "source": [
    "    $ pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-luther",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-treatment",
   "metadata": {},
   "source": [
    "## <span style=\"color:#124445\">Step 2. SentencePiece 모델 학습</span>\n",
    "---\n",
    "앞서 배운 **`tokenize()`** 함수를 기억하나요? 다시 한번 상기시켜드릴게요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-honduras",
   "metadata": {},
   "source": [
    "```{.python}\n",
    "def tokenize(corpus):  # corpus: Tokenized Sentence's List\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-season",
   "metadata": {},
   "source": [
    "위와 같이 **`tf.keras.preprocessing.text.Tokenizer`**에 corpus을 주고 **`tokenizer.fit_on_texts(corpus)`**을 하면 토크나이저 내부적으로 단어 사전과 토크나이저 기능을 corpus에 맞춤형으로 자동생성해 주는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-bunch",
   "metadata": {},
   "source": [
    "그럼 이를 위해서 SentencePiece 모델을 학습하는 과정을 거쳐야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-differential",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romance-event",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-64cdeba275be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_corpus\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# 이전 스텝에서 정제했던 corpus를 활용합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtered_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전 스텝에서 정제했던 corpus를 활용합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix=korean_spm --vocab_size={}'.format(temp_file, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = 'unigram'이 디폴트 적용되어 있습니다. --model_type = 'bpe' 로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l korean_spm*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-passion",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-virgin",
   "metadata": {},
   "source": [
    "위 코드를 실행하면 정상적으로 SentencePiece 모델 학습이 완료된 후 koreanspm.model 파일과 koreanspm.vocab vocabulary 파일이 생성되었음을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-plaintiff",
   "metadata": {},
   "source": [
    "그럼 이렇게 학습된 SentencePiece 모델을 어떻게 활용하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-nursing",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "respected-samuel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1245, 11, 306, 7, 3598, 11, 285, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-prefix",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-store",
   "metadata": {},
   "source": [
    "어떻습니까? SentencePiece의 토크나이징 실력이 괜찮은 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-wheel",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-apache",
   "metadata": {},
   "source": [
    "## <span style=\"color:#124445\">Step 3. Tokenizer 함수 작성</span>\n",
    "---\n",
    "우리는 위에서 훈련한 SentencePiece를 활용하여 위 함수와 유사한 기능을 하는 **`sp_tokenize()`** 함수를 정의할 겁니다. 하지만 SentencePiece가 동작하는 방식이 단순 토큰화와는 달라 완전히 동일하게는 정의하기 어렵습니다. 그러니 아래 조건을 만족하는 함수를 정의하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-curtis",
   "metadata": {},
   "source": [
    ">*1) 매개변수로 토큰화된 문장의 **`list`** 전달하는 대신 온전한 문장의 **`list`**를 전달합니다.*<br><br>\n",
    ">*2) **생성된 vocab 파일**을 읽어와 **`{ <word> : <idx> }`** 형태를 가지는 **`word_index`** 사전과 **`{ <idx> : <word>}`** 형태를 가지는 **`index_word`** 사전을 생성하고 함께 **반환**합니다.*<br><br>\n",
    ">*3) 반환값인 **`tensor`**는 앞의 함수와 같이 토큰화한 후 Encoding 된 문장입니다. 바로 학습에 사용할 수 있게 Padding은 당연히 해야겠죠?*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-mills",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "numerical-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_tokenize(s, corpus):\n",
    "\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(\"./korean_spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({idx:word})\n",
    "        index_word.update({word:idx})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dried-precipitation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-95eac78f0c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'나는 밥을 먹었습니다.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'그러나 여전히 ㅠㅠ 배가 고픕니다...'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c73c4cd09eba>\u001b[0m in \u001b[0;36msp_tokenize\u001b[0;34m(s, corpus)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mindex_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#sp_tokenize(s, corpus) 사용예제\n",
    "\n",
    "my_corpus = ['나는 밥을 먹었습니다.', '그러나 여전히 ㅠㅠ 배가 고픕니다...']\n",
    "tensor, word_index, index_word = sp_tokenize(s, my_corpus)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-crack",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-modification",
   "metadata": {},
   "source": [
    "## <span style=\"color:#124445\">Step 4. 네이버 영화리뷰 감정분석 문제에 SentencePiece 적용해 보기</span>\n",
    "---\n",
    "아마 여러분들은 [네이버 영화리뷰 감정분석 태스크](https://github.com/e9t/nsmc/)를 한 번쯤은 다루어 보았을 것입니다. 한국어로 된 corpus를 다루어야 하므로 주로 KoNLPy에서 제공하는 형태소 분석기를 사용하여 텍스트를 전처리해서 RNN 모델을 분류기로 사용했을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-ethiopia",
   "metadata": {},
   "source": [
    "만약 이 문제에서 tokenizer를 sentencepiece로 바꾸어 다시 풀어본다면 더 성능이 좋아질까요? 비교해 보는 것도 흥미로울 것입니다.\n",
    "\n",
    ">* 네이버 영화리뷰 감정분석 코퍼스에 sentencepiece를 적용한 모델 학습하기\n",
    ">*학습된 모델로 sp_tokenize() 메소드 구현하기\n",
    ">*구현된 토크나이저를 적용하여 네이버 영화리뷰 감정분석 모델을 재학습하기\n",
    ">*KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기\n",
    ">***(보너스)** SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기\n",
    ">*Word Vector는 활용할 필요가 없습니다. 활용이 가능하지도 않을 것입니다.\n",
    ">*머지않아 SentencePiece와 BERT 등의 pretrained 모델을 함께 활용하는 태스크를 다루게 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-search",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-hearing",
   "metadata": {},
   "source": [
    "## <span style=\"color:#124445\">Step 5. 루브릭 평가</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-sunrise",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|-------|--------|\n",
    "|1. SentencePiece를 이용하여 모델을 만들기까지의 과정이 정상적으로 진행되었는가?|코퍼스 분석, 전처리, SentencePiece 적용, 토크나이저 구현 및 동작이 빠짐없이 진행되었는가?|\n",
    "|2. SentencePiece를 통해 만든 Tokenizer가 자연어처리 모델과 결합하여 동작하는가?|SentencePiece 토크나이저가 적용된 Text Classifier 모델이 정상적으로 수렴하여 80% 이상의 test accuracy가 확인되었다.|\n",
    "|3. SentencePiece의 성능을 다각도로 비교·분석하였는가?|SentencePiece 토크나이저를 활용했을 때의 성능을 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교하여 분석을 체계적으로 진행하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-equilibrium",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-shore",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 6. 회고</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-washington",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-ready",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
