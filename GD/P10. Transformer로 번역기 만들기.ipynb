{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-nancy",
   "metadata": {},
   "source": [
    "# <center><span style=\"color:#2C786C\">í”„ë¡œì íŠ¸ : ë” ë©‹ì§„ ë²ˆì—­ê¸° ë§Œë“¤ê¸°</span></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-accused",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-puppy",
   "metadata": {},
   "source": [
    "## <span style=\"color:#F7B400\">ìˆœì„œ</span>\n",
    "> **<span style=\"color:#2C786C\">Step 1-1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ë¡œì»¬ ìœ ì €ìš©)</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 1-2. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í´ë¼ìš°ë“œ ìœ ì €ìš©)</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 2. ë°ì´í„° ì •ì œ ë° í† í°í™”</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 3. ëª¨ë¸ ì„¤ê³„</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 4. í›ˆë ¨í•˜ê¸°</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 5. ì •ë¦¬</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 6. ë£¨ë¸Œë¦­ í‰ê°€</span>**<br>\n",
    "**<span style=\"color:#2C786C\">Step 7. íšŒê³ </span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-rhythm",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-branch",
   "metadata": {},
   "source": [
    "### <span style=\"color:#926DD6\">ì°¸ê³ </span>\n",
    ">* []()\n",
    ">* []()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-retirement",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-professor",
   "metadata": {},
   "source": [
    "### <span style=\"color:#926DD6\">ê°œë…</span>\n",
    ">*\n",
    "\n",
    ">*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-induction",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-justice",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 1-1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ë¡œì»¬ ìœ ì €ìš©)</span>\n",
    "---\n",
    "ì•„ë˜ ë§í¬ì—ì„œ korean-english-park.train.tar.gz ë¥¼ ë‹¤ìš´ë¡œë“œë°›ì•„ í•œì˜ ë³‘ë ¬ ë°ì´í„°ë¥¼ í™•ë³´í•©ë‹ˆë‹¤.\n",
    "\n",
    "* [jungyeul/korean-parallel-corpora](https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-defensive",
   "metadata": {},
   "source": [
    "*ğŸ’¡ì´ì „ [ Seq2seqìœ¼ë¡œ ë²ˆì—­ê¸° ë§Œë“¤ê¸° ] ì½”ìŠ¤ì—ì„œ ì‚¬ìš©í•œ ë°ì´í„°ì™€ ë™ì¼í•œ ë°ì´í„°ì…ë‹ˆë‹¤!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-spirituality",
   "metadata": {},
   "source": [
    "í„°ë¯¸ë„ì„ ì—´ì–´ì„œ í•˜ë‹¨ì˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-newark",
   "metadata": {},
   "source": [
    "    $ mkdir -p ~/aiffel/transformer/data\n",
    "    \n",
    "    $ cd ~/aiffel/transformer/data\n",
    "\n",
    "    $ wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
    "    \n",
    "    $ gzip -d korean-english-park.train.tar.gz\n",
    "    \n",
    "    $ tar -xvf korean-english-park.train.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-allah",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-destruction",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 1-2. ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ë¡œì»¬ ìœ ì €ìš©)</span>\n",
    "---\n",
    "â˜ï¸í´ë¼ìš°ë“œë¥¼ ì´ìš©ì¤‘ì´ì‹  ë¶„ì€ ìš°ì¸¡í•˜ë‹¨ì˜ Cloud shellì„ ì—´ì–´ì£¼ì„¸ìš”.\n",
    "ì•„ë˜ì™€ ê°™ì´ ê³µìœ ë””ë ‰í† ë¦¬ì— ì €ì¥ëœ ë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” ì‹¬ë³¼ë¦­ ë§í¬ë¥¼ ìƒì„±í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-lesson",
   "metadata": {},
   "source": [
    "    $ ln -s ~/data ~/aiffel/transformer/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-pattern",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-letter",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 2. ì¶”ì¶œëœ ê²°ê³¼ë¡œ embedding model ë§Œë“¤ê¸°</span>\n",
    "---\n",
    "1) **`set`** ë°ì´í„°í˜•ì´ **ì¤‘ë³µì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ í™œìš©**í•´ ì¤‘ë³µëœ ë°ì´í„°ë¥¼ ì œê±°í•˜ë„ë¡ í•©ë‹ˆë‹¤. ë°ì´í„°ì˜ **ë³‘ë ¬ ìŒì´ ííŠ¸ëŸ¬ì§€ì§€ ì•Šê²Œ ì£¼ì˜**í•˜ì„¸ìš”! ì¤‘ë³µì„ ì œê±°í•œ ë°ì´í„°ë¥¼ **`cleaned_corpus`**ì— ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-groove",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "second-accident",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-837e0548f5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HOME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/aiffel/transformer/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkor_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/korean-english-park.train.ko\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meng_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/korean-english-park.train.en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ë°ì´í„° ì •ì œ ë° í† í°í™”\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# ë°ì´í„° ì •ì œ ë° í† í°í™”\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    assert len(kor) == len(eng)\n",
    "\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-interface",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-dance",
   "metadata": {},
   "source": [
    "2) ì •ì œ í•¨ìˆ˜ë¥¼ ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ê²Œ ì •ì˜í•˜ì„¸ìš”.\n",
    "> ì¡°ê±´\n",
    ">* *ëª¨ë“  ì…ë ¥ì„ **ì†Œë¬¸ìë¡œ ë³€í™˜**í•©ë‹ˆë‹¤.*\n",
    ">* ***ì•ŒíŒŒë²³, ë¬¸ì¥ë¶€í˜¸, í•œê¸€**ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.*\n",
    ">* ***ë¬¸ì¥ë¶€í˜¸ ì–‘ì˜†ì— ê³µë°±**ì„ ì¶”ê°€í•©ë‹ˆë‹¤.*\n",
    ">* *ë¬¸ì¥ ì•ë’¤ì˜ **ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°**í•©ë‹ˆë‹¤.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-reggae",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-thousand",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-action",
   "metadata": {},
   "source": [
    "2) ì •ì œ í•¨ìˆ˜ë¥¼ ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ê²Œ ì •ì˜í•˜ì„¸ìš”.\n",
    "> ì¡°ê±´\n",
    ">* *ëª¨ë“  ì…ë ¥ì„ **ì†Œë¬¸ìë¡œ ë³€í™˜**í•©ë‹ˆë‹¤.*\n",
    ">* ***ì•ŒíŒŒë²³, ë¬¸ì¥ë¶€í˜¸, í•œê¸€*** *ë§Œ ë‚¨ê¸°ê³  ëª¨ë‘ ì œê±°í•©ë‹ˆë‹¤.*\n",
    ">* ***ë¬¸ì¥ë¶€í˜¸ ì–‘ì˜†ì— ê³µë°±*** *ì„ ì¶”ê°€í•©ë‹ˆë‹¤.*\n",
    ">* *ë¬¸ì¥ ì•ë’¤ì˜ **ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì œê±°**í•©ë‹ˆë‹¤.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-royal",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-memphis",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-sector",
   "metadata": {},
   "source": [
    "3) í•œê¸€ ë§ë­‰ì¹˜ **`kor_corpus`**ì™€ ì˜ë¬¸ ë§ë­‰ì¹˜ **`eng_corpus`**ë¥¼ ê°ê° ë¶„ë¦¬í•œ í›„, **ì •ì œí•˜ì—¬ í† í°í™”ë¥¼ ì§„í–‰**í•©ë‹ˆë‹¤! **í† í°í™”ì—ëŠ” *Sentencepiece*ë¥¼ í™œìš©**í•˜ì„¸ìš”. ì²¨ë¶€ëœ ê³µì‹ ì‚¬ì´íŠ¸ë¥¼ ì°¸ê³ í•´ ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” **`generate_tokenizer()`** í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ **`ko_tokenizer`** ê³¼ **`en_tokenizer`** ë¥¼ ì–»ìœ¼ì„¸ìš”. **`en_tokenizer`**ì—ëŠ” **`set_encode_extra_options(\"bos:eos\")`** í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ **íƒ€ê²Ÿ ì…ë ¥ì´ ë¬¸ì¥ì˜ ì‹œì‘ í† í°ê³¼ ë í† í°ì„ í¬í•¨í•  ìˆ˜ ìˆê²Œ** í•©ë‹ˆë‹¤.\n",
    "> ì¡°ê±´\n",
    ">* *í•™ìŠµ í›„ ì €ì¥ëœ **`model`** íŒŒì¼ì„ **`SentencePieceProcessor()`** í´ë˜ìŠ¤ì— **`Load()`**í•œ í›„ ë°˜í™˜í•©ë‹ˆë‹¤.*\n",
    ">* ***íŠ¹ìˆ˜ í† í°ì˜ ì¸ë±ìŠ¤*** *ë¥¼ ì•„ë˜ì™€ ë™ì¼í•˜ê²Œ ì§€ì •í•©ë‹ˆë‹¤.<br>**`<PAD>`** : 0 / **`<BOS>`** : 1 / **`<EOS>`** : 2 / **`<UNK>`** : 3*\n",
    "\n",
    "\n",
    "* ì°¸ê³ : [google/sentencepiece](https://github.com/google/sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-objective",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentencepieceë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµí•œ tokenizerë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-brush",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-powder",
   "metadata": {},
   "source": [
    "4) í† í¬ë‚˜ì´ì €ë¥¼ í™œìš©í•´ **í† í°ì˜ ê¸¸ì´ê°€ 50 ì´í•˜**ì¸ ë°ì´í„°ë¥¼ ì„ ë³„í•˜ì—¬ **`src_corpus`** ì™€ **`tgt_corpus`** ë¥¼ ê°ê° êµ¬ì¶•í•˜ê³ , í…ì„œ **`enc_train`** ê³¼ **`dec_train`** ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”! (â—ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê²½ìš° í•™ìŠµì— êµ‰ì¥íˆ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-while",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook    # Process ê³¼ì •ì„ ë³´ê¸° ìœ„í•´\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# í† í°ì˜ ê¸¸ì´ê°€ 50 ì´í•˜ì¸ ë¬¸ì¥ë§Œ ë‚¨ê¹ë‹ˆë‹¤. \n",
    "for idx in tqdm_notebook(range(len(kor_corpus))):\n",
    "    # [[YOUR CODE]]\n",
    "\n",
    "# íŒ¨ë”©ì²˜ë¦¬ë¥¼ ì™„ë£Œí•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ë¥¼ ì™„ì„±í•©ë‹ˆë‹¤. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-signature",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-leone",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 3. ëª¨ë¸ ì„¤ê³„</span>\n",
    "---\n",
    "ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©ì„ í™œìš©í•´ì„œ **`Transformer`** ëª¨ë¸ì„ ì„¤ê³„í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-bermuda",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-calcium",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 4. í›ˆë ¨í•˜ê¸°</span>\n",
    "---\n",
    "ì•ì„œ í•„ìš”í•œ ê²ƒë“¤ì„ ëª¨ë‘ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” í›ˆë ¨ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤! ì•„ë˜ ê³¼ì •ì„ ì°¨ê·¼ì°¨ê·¼ ë”°ë¼ê°€ë©° ëª¨ë¸ì„ í›ˆë ¨í•˜ê³ , **ì˜ˆë¬¸ì— ëŒ€í•œ ë©‹ì§„ ë²ˆì—­**ì„ ì œì¶œí•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-drive",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-bibliography",
   "metadata": {},
   "source": [
    "1. **2 Layer**ë¥¼ ê°€ì§€ëŠ” **`Transformer`**ë¥¼ ì„ ì–¸í•˜ì„¸ìš”.\n",
    "(í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ììœ ë¡­ê²Œ ì¡°ì ˆí•©ë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-ambassador",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-queue",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-stroke",
   "metadata": {},
   "source": [
    "2. ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ ê²ƒê³¼ ë™ì¼í•œ **Learning Rate Scheduler**ë¥¼ ì„ ì–¸í•˜ê³ , ì´ë¥¼ í¬í•¨í•˜ëŠ” **Adam Optimizer**ë¥¼ ì„ ì–¸í•˜ì„¸ìš”. (Optimizerì˜ íŒŒë¼ë¯¸í„° ì—­ì‹œ ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-celebrity",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = # [[YOUR CODE]]\n",
    "optimizer = # [[YOUR CODE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-capability",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-interaction",
   "metadata": {},
   "source": [
    "3. **Loss í•¨ìˆ˜ë¥¼ ì •ì˜**í•˜ì„¸ìš”.\n",
    "Sequence-to-sequence ëª¨ë¸ì—ì„œ ì‚¬ìš©í–ˆë˜ Lossì™€ ìœ ì‚¬í•˜ë˜, **Masking ë˜ì§€ ì•Šì€ ì…ë ¥ì˜ ê°œìˆ˜ë¡œ Scaling**í•˜ëŠ” ê³¼ì •ì„ ì¶”ê°€í•©ë‹ˆë‹¤. (íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ëª¨ë“  ì…ë ¥ì— ëŒ€í•œ Lossë¥¼ í•œ ë²ˆì— êµ¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-senate",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking ë˜ì§€ ì•Šì€ ì…ë ¥ì˜ ê°œìˆ˜ë¡œ Scalingí•˜ëŠ” ê³¼ì •\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-condition",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-elevation",
   "metadata": {},
   "source": [
    "4. **`train_step`** í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì„¸ìš”. **ì…ë ¥ ë°ì´í„°ì— ì•Œë§ì€ Maskë¥¼ ìƒì„±**í•˜ê³ , ì´ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ì—°ì‚°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-visit",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step í•¨ìˆ˜ ì •ì˜\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # ê³„ì‚°ëœ lossì— tf.GradientTape()ë¥¼ ì ìš©í•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # ìµœì¢…ì ìœ¼ë¡œ optimizer.apply_gradients()ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. \n",
    "    # [[YOUR CODE]]\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-conditions",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-practitioner",
   "metadata": {},
   "source": [
    "5. **í•™ìŠµì„ ì§„í–‰**í•©ë‹ˆë‹¤. **ë§¤ Epoch ë§ˆë‹¤ ì œì‹œëœ ì˜ˆë¬¸ì— ëŒ€í•œ ë²ˆì—­ì„ ìƒì„±**í•˜ê³ , ë©‹ì§„ ë²ˆì—­ì´ ìƒì„±ë˜ë©´ ê·¸ë•Œì˜ **í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ìƒì„±ëœ ë²ˆì—­ì„ ì œì¶œ**í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-thing",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-apartment",
   "metadata": {},
   "source": [
    "ì˜ˆë¬¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-track",
   "metadata": {},
   "source": [
    "    1. ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤.\n",
    "    2. ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤.\n",
    "    3. ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤.\n",
    "    4. ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-minister",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-socket",
   "metadata": {},
   "source": [
    "ê²°ê³¼(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-metadata",
   "metadata": {},
   "source": [
    "    Translations\n",
    "    > 1. obama is the president elect .\n",
    "    > 2. they are in the city .\n",
    "    > 3. they don t need to be a lot of drink .\n",
    "    > 4. seven other people have been killed in the attacks .\n",
    "\n",
    "    Hyperparameters\n",
    "    > n_layers: 2\n",
    "    > d_model: 512\n",
    "    > n_heads: 8\n",
    "    > d_ff: 2048\n",
    "    > dropout: 0.3\n",
    "\n",
    "    Training Parameters\n",
    "    > Warmup Steps: 4000\n",
    "    > Batch Size: 64\n",
    "    > Epoch At: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-permit",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-assets",
   "metadata": {},
   "source": [
    "ë²ˆì—­ ìƒì„±ì—ëŠ” ì•„ë˜ ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention ì‹œê°í™” í•¨ìˆ˜\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ìƒì„± í•¨ìˆ˜\n",
    "\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "    \n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
    "        generate_masks(_input, output)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
    "        model(_input, \n",
    "              output,\n",
    "              enc_padding_mask,\n",
    "              combined_mask,\n",
    "              dec_padding_mask)\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²ˆì—­ ìƒì„± ë° Attention ì‹œê°í™” ê²°í•©\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-wilderness",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-hunter",
   "metadata": {},
   "source": [
    "**`translate()`** í•¨ìˆ˜ì˜ **`plot_attention`** ë³€ìˆ˜ë¥¼ **`True`** ë¡œ ì£¼ë©´ **ë²ˆì—­ ê²°ê³¼ì— ëŒ€í•œ Attention Mapì„ ì‹œê°í™”** í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-baltimore",
   "metadata": {},
   "source": [
    "ğŸ’¡ ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ ì œì‹œí•œ ì˜ˆë¬¸ì€ **Seq2seqìœ¼ë¡œ ë²ˆì—­ê¸° ë§Œë“¤ê¸°**ì˜ ì˜ˆë¬¸ê³¼ ë™ì¼í•©ë‹ˆë‹¤. Seq2seqê³¼ Transformerë¡œ ë§Œë“  ë‘ ë²ˆì—­ê¸°ì˜ ì„±ëŠ¥ì„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì • ë“± ë‹¤ì–‘í•œ ì—°êµ¬í•´ë³´ì‹œë©´ í•™ìŠµì— ë„ì›€ì´ ë˜ì‹¤ ê±°ì˜ˆìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-powder",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ, í•™ìŠµì˜ ì „ ê³¼ì •ì„ êµ¬í˜„í•œ ì½”ë“œë¥¼ ì²¨ë¶€í•©ë‹ˆë‹¤. êµ¬í˜„ê³¼ì •ì— ì°¸ê³ í•´ ì£¼ì„¸ìš”. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-virus",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-northwest",
   "metadata": {},
   "source": [
    "    # í•™ìŠµ ê³¼ì • ì˜ˆì‹œ\n",
    "\n",
    "    from tqdm import tqdm_notebook \n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 20\n",
    "\n",
    "    examples = [\n",
    "                \"ì˜¤ë°”ë§ˆëŠ” ëŒ€í†µë ¹ì´ë‹¤.\",\n",
    "                \"ì‹œë¯¼ë“¤ì€ ë„ì‹œ ì†ì— ì‚°ë‹¤.\",\n",
    "                \"ì»¤í”¼ëŠ” í•„ìš” ì—†ë‹¤.\",\n",
    "                \"ì¼ê³± ëª…ì˜ ì‚¬ë§ìê°€ ë°œìƒí–ˆë‹¤.\"\n",
    "    ]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm_notebook(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                        dec_train[idx:idx+BATCH_SIZE],\n",
    "                        transformer,\n",
    "                        optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "        for example in examples:\n",
    "            translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-wyoming",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-lodge",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 5. ì •ë¦¬</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-tampa",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-birmingham",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 6. ë£¨ë¸Œë¦­ í‰ê°€</span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-concentrate",
   "metadata": {},
   "source": [
    "|í‰ê°€ë¬¸í•­|ìƒì„¸ê¸°ì¤€|\n",
    "|-------|--------|\n",
    "|1. ë²ˆì—­ê¸° ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ê°€ ì˜ ì´ë£¨ì–´ì¡Œë‹¤.|ë°ì´í„° ì •ì œ, SentencePieceë¥¼ í™œìš©í•œ í† í°í™” ë° ë°ì´í„°ì…‹ êµ¬ì¶•ì˜ ê³¼ì •ì´ ì§€ì‹œëŒ€ë¡œ ì§„í–‰ë˜ì—ˆë‹¤.|\n",
    "|2. Transformer ë²ˆì—­ê¸° ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ êµ¬ë™ëœë‹¤.|Transformer ëª¨ë¸ì˜ í•™ìŠµê³¼ ì¶”ë¡  ê³¼ì •ì´ ì •ìƒì ìœ¼ë¡œ ì§„í–‰ë˜ì–´, í•œ-ì˜ ë²ˆì—­ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•œë‹¤.|\n",
    "|3. í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜ë¯¸ê°€ í†µí•˜ëŠ” ìˆ˜ì¤€ì˜ ë²ˆì—­ë¬¸ì´ ìƒì„±ë˜ì—ˆë‹¤.|ì œì‹œëœ ë¬¸ì¥ì— ëŒ€í•œ ê·¸ëŸ´ë“¯í•œ ì˜ì–´ ë²ˆì—­ë¬¸ì´ ìƒì„±ë˜ë©°, ì‹œê°í™”ëœ Attention Mapìœ¼ë¡œ ê²°ê³¼ë¥¼ ë’·ë°›ì¹¨í•œë‹¤.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-trunk",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-bulgarian",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2C786C\">Step 7. íšŒê³ </span>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-episode",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
