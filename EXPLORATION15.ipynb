{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ ë§Œë“œëŠ” ëŒ€í™”í˜• ì±—ë´‡\n",
    "-----\n",
    "### í”„ë¡œì íŠ¸ - í•œêµ­ì–´ ë°ì´í„°ë¡œ ì±—ë´‡ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì±—ë´‡ì„ ì œì‘í•´ ë³´ë ¤ê³  í•©ë‹ˆë‹¤.<br>\n",
    "ë§ì€ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ë“¤ì€ í…ìŠ¤íŠ¸ ë¬¸ì¥ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê¸° ìœ„í•´ ë‹¨ì–´ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë²¡í„°í™” ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ë˜í•œ ê·¸ ì ì—ì„œëŠ” ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ ë‹¤ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.<br>í•˜ì§€ë§Œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì˜ ì…ë ¥ë°ì´í„° ì²˜ë¦¬ì—ëŠ” RNN ê³„ì—´ì˜ ëª¨ë¸ë“¤ê³¼ ë‹¤ë¥¸ ì ì´ í•œ ê°€ì§€ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ ì„ë² ë”© ë²¡í„°ì— ì–´ë–¤ ê°’ì„ ë”í•´ì¤€ ë’¤ì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.<br>ê·¸ ê°’ì€ ë°”ë¡œ ì•„ë˜ì˜ **`í¬ì§€ì…”ë„ ì¸ì½”ë”©(positional Encoding)`**ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.![](https://aiffelstaticprd.blob.core.windows.net/media/original_images/Untitled_4_fuzN6PD.png)<br>\n",
    "**íŠ¸ëœìŠ¤í¬ë¨¸ê°€ RNNê³¼ ê²°ì •ì ìœ¼ë¡œ ë‹¤ë¥¸ ì ì´ ë°”ë¡œ ì´ ë¶€ë¶„ì…ë‹ˆë‹¤.**<br>ë¬¸ì¥ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ í•œêº¼ë²ˆì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì…ë ¥ë°›ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ìì¹« 'I ate lunch'ì™€ 'lunch ate I'ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ì„ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤.<br>ê·¸ë˜ì„œ ê°™ì€ ë‹¨ì–´ë¼ë„ ê·¸ ë‹¨ì–´ê°€ ë¬¸ì¥ì˜ ëª‡ ë²ˆì§¸ ì–´ìˆœìœ¼ë¡œ ì…ë ¥ë˜ì—ˆëŠ”ì§€ë¥¼ ëª¨ë¸ì— ì¶”ê°€ë¡œ ì•Œë ¤ ì£¼ê¸° ìœ„í•´, ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ì—ë‹¤ê°€ ìœ„ì¹˜ ì •ë³´ë¥¼ ê°€ì§„ ë²¡í„°(Positional Encoding) ê°’ì„ ë”í•´ì„œ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¼ëŠ” ê²ƒì´ì§€ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹¤ìŠµëª©í‘œ\n",
    ">* íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë” ë””ì½”ë” êµ¬ì¡° ì´í•´í•˜ê¸°\n",
    ">* ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ê¸°\n",
    ">* ì…€í”„ ì–´í…ì…˜ ì´í•´í•˜ê¸°\n",
    ">* í•œêµ­ì–´ì—ë„ ì ìš©í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆœì„œ\n",
    "> **Step1.** ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°<br>\n",
    "**Step2.** ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°<br>\n",
    "**Step3.** SubwordTextEncoder ì‚¬ìš©í•˜ê¸°<br>\n",
    "**Step4.** ëª¨ë¸ êµ¬ì„±í•˜ê¸°<br>\n",
    "**Step5.** ëª¨ë¸ í‰ê°€í•˜ê¸°<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê°„ë‹¨ì •ë¦¬! \n",
    "---\n",
    "ì°¸ê³ ìë£Œ : [ì±—ë´‡ì˜ 5ê°€ì§€ ëŒ€í‘œ ìœ í˜•](https://tonyaround.com/%ec%b1%97%eb%b4%87-%ea%b8%b0%ed%9a%8d-%eb%8b%a8%ea%b3%84-%ec%b1%97%eb%b4%87%ec%9d%98-5%ea%b0%80%ec%a7%80-%eb%8c%80%ed%91%9c-%ec%9c%a0%ed%98%95-%ec%a2%85%eb%a5%98/)\n",
    "- **ëŒ€í™”í˜• ì±—ë´‡** : ìì—°ì–´ì²˜ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”ê°€ ê°€ëŠ¥(ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ì„ ê¸°ë³¸)<br>\n",
    "- **íŠ¸ë¦¬í˜•(ë²„íŠ¼) ì±—ë´‡** : ì •í•´ì§„ íŠ¸ë¦¬êµ¬ì¡°ë¥¼ ë”°ë¼ ë‹µë³€ì„ ì–»ëŠ” í˜•íƒœ, ì±—ë´‡ì˜ 1ì„¸ëŒ€ì—ì„œ ë§ì´ ì¼ë˜ ë°©ì‹(ì¸ê³µì§€ëŠ¥ X)\n",
    "![íŠ¸ë¦¬í˜• ì±—ë´‡](https://i2.wp.com/tonyaround.com/wp-content/uploads/2019/09/unnamed-file.png?resize=1024%2C533&ssl=1)\n",
    "\n",
    "- **ì¶”ì²œí˜• ì±—ë´‡** : í‘œë©´ì ìœ¼ë¡œëŠ” ëŒ€í™”ì˜ í˜•íƒœë¥¼ ë„ê³  ìˆì§€ë§Œ, ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë°©ì‹ì´ ëŒ€í™”í˜• ì±—ë´‡ê³¼ ë‹¤ë¦„, ëŒ€í™”í˜• ì±—ë´‡ìœ¼ë¡œ ê°€ê¸° ìœ„í•œ ì¤‘ê°„ ê³¼ì •ìœ¼ë¡œë„ ì‚¬ìš©\n",
    "![ì¶”ì²œí˜• ì±—ë´‡](https://i2.wp.com/tonyaround.com/wp-content/uploads/2019/09/unnamed-file-2.png?w=695&ssl=1)\n",
    "\n",
    "- **ì‹œë‚˜ë¦¬ì˜¤í˜• ì±—ë´‡** : ì›í•˜ëŠ” ì„œë¹„ìŠ¤ í˜¹ì€ ì•„ì›ƒí’‹ ì œê³µì„ ìœ„í•˜ì—¬ ì •í•´ì§„ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìˆ˜í–‰í•´ì£¼ëŠ” ì±—ë´‡, ê°€ì¥ íˆ¬ìëŒ€ë¹„ íš¨ê³¼ê°€ ì¢‹ì€ ì±—ë´‡ ìœ í˜•\n",
    "![ì‹œë‚˜ë¦¬ì˜¤í˜• ì±—ë´‡](https://i2.wp.com/tonyaround.com/wp-content/uploads/2019/09/unnamed-file-3.png?w=893&ssl=1)\n",
    "\n",
    "- **ê²°í•©í˜• ì±—ë´‡** : ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë”°ë¼ ìœ„ì˜ ì±—ë´‡ ìœ í˜•ë“¤ì„ ê²°í•©í•´ì„œ ì„¤ê³„ê°€ ê°€ëŠ¥\n",
    "![ê²°í•©í˜• ì±—ë´‡](https://i2.wp.com/tonyaround.com/wp-content/uploads/2019/09/unnamed-file-4.png?resize=1024%2C557&ssl=1)\n",
    "\n",
    "ëŒ€í™”í˜•ì„ ì œì™¸í•˜ë©´ ì‚¬ì‹¤ìƒ ì±—ë´‡ì€ ëŒ€í™”í˜• UXë¥¼ ê°€ì¡Œì§€ë§Œ ë³¸ì§ˆì ìœ¼ë¡œëŠ” ê²€ìƒ‰ì—”ì§„ì´ê±°ë‚˜, í˜¹ì€ ìŒì„±ARSë¥¼ ëŒ€í™”í˜• UXì— ì˜®ê²¨ë†“ì€ ê²ƒì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¸ì½”ë”ì™€ ë””ì½”ë” êµ¬ì¡° ë˜ì§šì–´ë³´ê¸°\n",
    "---\n",
    "![ì¸ì½”ë”ì™€ ë””ì½”ë” êµ¬ì¡°](https://aiffelstaticprd.blob.core.windows.net/media/images/Untitled_UcFQAjh.max-800x600.png)\n",
    "\n",
    "ë²ˆì—­ê¸°ë¥¼ ë§Œë“œëŠ” ë° ì‚¬ìš©í•œ ëŒ€í‘œì ì¸ ëª¨ë¸ì¸ ì¸ì½”ë”ì™€ ë””ì½”ë” êµ¬ì¡°ë¥¼ ë˜ì§šì–´ ë´…ì‹œë‹¤.<br>ì¸ì½”ë”ì— ì…ë ¥ ë¬¸ì¥ì´ ë“¤ì–´ê°€ê³ , ë””ì½”ë”ëŠ” ì´ì— ìƒì‘í•˜ëŠ” ì¶œë ¥ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.<br>ì´ë¥¼ í›ˆë ¨í•œë‹¤ëŠ” ê²ƒì€ ê²°êµ­ ì…ë ¥ ë¬¸ì¥ê³¼ ì¶œë ¥ ë¬¸ì¥ ë‘ ê°€ì§€ ë³‘ë ¬ êµ¬ì¡°ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ í›ˆë ¨í•œë‹¤ëŠ” ì˜ë¯¸ì˜€ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ êµ¬ì„±(ë²ˆì—­)**\n",
    "- ì…ë ¥ ë¬¸ì¥ : 'ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤.'\n",
    "- ì¶œë ¥ ë¬¸ì¥ : 'I am a student'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ í›ˆë ¨ ë°ì´í„°ì…‹ì˜ êµ¬ì„±(ì§ˆë¬¸-ë‹µë³€)**\n",
    "- ì…ë ¥ ë¬¸ì¥ : 'ì˜¤ëŠ˜ì˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?'\n",
    "- ì¶œë ¥ ë¬¸ì¥ : 'ì˜¤ëŠ˜ì€ ë§¤ìš° í™”ì°½í•œ ë‚ ì”¨ì•¼'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ì™€ ë””ì½”ë”\n",
    "---\n",
    "ë§ˆì°¬ê°€ì§€ë¡œ ì…ë ¥ ë¬¸ì¥ì„ ë„£ìœ¼ë©´ ì¶œë ¥ ë¬¸ì¥ì„ ë‚´ë±‰ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.\n",
    "![íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì¸ì½”ë”ì™€ ë””ì½”ë” êµ¬ì¡°](https://aiffelstaticprd.blob.core.windows.net/media/images/Untitled_1_kxflIxg.max-800x600.png)<br>\n",
    "\n",
    "ìœ„ì˜ ë¸”ë™ë°•ìŠ¤ë¡œ ê°€ë ¤ì ¸ ìˆëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë‚´ë¶€êµ¬ì¡°ë¥¼ ì—´ì–´ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤!\n",
    "![ë‚´ë¶€êµ¬ì¡°](https://aiffelstaticprd.blob.core.windows.net/media/images/Untitled_2_EnQyi4S.max-800x600.png)<br>\n",
    "ê·¸ë¦¬ê³  ê·¸ ë‚´ë¶€ë¥¼ ì¡°ê¸ˆ ë” í™•ëŒ€í•´ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ í†±ë‹ˆë°”í€´ì²˜ëŸ¼ ë§ë¬¼ë ¤ ëŒì•„ê°€ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë¶€í’ˆë“¤ë¡œ êµ¬ì„±ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "![ì„¸ë¶€ë‚´ë¶€êµ¬ì¡°](https://aiffelstaticprd.blob.core.windows.net/media/images/Untitled_3_ddZedfW.max-800x600.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ë¬¼\n",
    "---\n",
    "í„°ë¯¸ë„ì„ ì—´ì–´ ì‹¤ìŠµì— í•„ìš”í•œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "> $ mkdir -p ~/aiffel/songys_chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°\n",
    "---\n",
    "í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°ëŠ” ì†¡ì˜ìˆ™ë‹˜ì´ ê³µê°œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë°ì´í„°ëŠ” ì•„ë˜ì˜ ë§í¬ì—ì„œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[songys/Chatbot_data](https://github.com/songys/Chatbot_data/blob/master/ChatbotData%20.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`wgetìœ¼ë¡œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ`**\n",
    "\n",
    ">$ wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData%20.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ ì‚¬ìš©í•  íŒ¨í‚¤ì§€(ë¼ì´ë¸ŒëŸ¬ë¦¬)ë“¤ì„ import í•´ë³´ê² ìŠµë‹ˆë‹¤.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Duplicate key in file PosixPath('/home/aiffel-dj49/anaconda3/envs/aiffel/lib/python3.7/site-packages/matplotlib/mpl-data/matplotlibrc'), line 250 ('font.family: NanumGothic')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ChatbotData .csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-901b59b78d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ChatbotData .csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             )\n\u001b[1;32m   1044\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1863\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m             )\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ChatbotData .csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('ChatbotData .csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\", filename=\"ChatBotData.csv\")\n",
    "train_data = pd.read_csv('ChatBotData.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ì±—ë´‡ ìƒ˜í”Œì˜ ê°œìˆ˜ :', len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "---\n",
    "ì˜ì–´ ë°ì´í„°ì™€ëŠ” ì „í˜€ ë‹¤ë¥¸ ë°ì´í„°ì¸ ë§Œí¼ ì˜ì–´ ë°ì´í„°ì— ì‚¬ìš©í–ˆë˜ ì „ì²˜ë¦¬ì™€ ì¼ë¶€ ë™ì¼í•œ ì „ì²˜ë¦¬ë„ í•„ìš”í•˜ê² ì§€ë§Œ ì „ì²´ì ìœ¼ë¡œëŠ” ë‹¤ë¥¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.<br>ì´ë²ˆ ì „ì²˜ë¦¬ëŠ” ì •ê·œ í‘œí˜„ì‹(Regular Expression)ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ë‘ì (punctuation)ì„ ì œê±°í•˜ì—¬ ë‹¨ì–´ë¥¼ í† í¬ë‚˜ì´ì§•(tokenizing)í•˜ëŠ” ì¼ì— ë°©í•´ê°€ ë˜ì§€ ì•Šë„ë¡ ì •ì œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜\n",
    "MAX_SAMPLES = 50000\n",
    "print('ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜', MAX_SAMPLES, 'ê°œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    # ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    # ì˜ˆë¥¼ ë“¤ì–´ì„œ \"I am a student.\" => \"I am a student .\"ì™€ ê°™ì´\n",
    "    # studentì™€ ì˜¨ì  ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_conversations():\n",
    "    id2line = {}\n",
    "    with open(path_to_movie_lines, errors='ignore') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        id2line[parts[0]] = parts[4]\n",
    "\n",
    "    inputs, outputs = [], []\n",
    "    with open(path_to_movie_conversations, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "        conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "\n",
    "    for i in range(len(conversation) - 1):  # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©.\n",
    "        inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "        outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "\n",
    "        if len(inputs) >= MAX_SAMPLES:\n",
    "            return inputs, outputs\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "questions, answers = load_conversations()\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(questions), 'ê°œ')\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(answers), 'ê°œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. SubwordTextEncoder ì‚¬ìš©í•˜ê¸°\n",
    "---\n",
    "í•œêµ­ì–´ ë°ì´í„°ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ í† í¬ë‚˜ì´ì§•ì„ í•´ì•¼ í•œë‹¤ê³  ë§ì€ ë¶„ì´ ì•Œê³  ìˆìŠµë‹ˆë‹¤.<br>í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì•„ë‹Œ ìœ„ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í–ˆë˜ ë‚´ë¶€ ë‹¨ì–´ í† í¬ë‚˜ì´ì €ì¸ **`SubwordTextEncoder`**ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TensorFlow Datasets **SubwordTextEncoder**ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ ì‚¬ìš©í•œë‹¤.  ë‹¨ì–´ë³´ë‹¤ ë” ì‘ì€ ë‹¨ìœ„ì¸ Subwordë¥¼ ê¸°ì¤€ìœ¼ë¡œ í† í¬ë‚˜ì´ì§•í•˜ê³ ,  ê° í† í°ì„ ê³ ìœ í•œ **ì •ìˆ˜ë¡œ ì¸ì½”ë”©**í•œë‹¤.\n",
    "2. ê° ë¬¸ì¥ì„ í† í°í™”í•˜ê³  ê° ë¬¸ì¥ì˜ ì‹œì‘ê³¼ ëì„ ë‚˜íƒ€ë‚´ëŠ” **`START_TOKEN`** ë° **`END_TOKEN`**ì„ ì¶”ê°€í•œë‹¤.\n",
    "3. ìµœëŒ€ ê¸¸ì´ **MAX_LENGTH**ì¸ 40ì„ ë„˜ëŠ” ë¬¸ì¥ë“¤ì€ í•„í„°ë§í•œë‹¤.\n",
    "4. MAX_LENGTHë³´ë‹¤ ê¸¸ì´ê°€ ì§§ì€ ë¬¸ì¥ë“¤ì€ 40ì— ë§ë„ë¡ **íŒ¨ë”©**í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **âœ“ ë‹¨ì–´ì¥(Vocabulary) ë§Œë“¤ê¸°**\n",
    "---\n",
    "ìš°ì„  ê° ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ë¶€ì—¬í•˜ê¸° ìœ„í•´ì„œ ë‹¨ì–´ì¥(Vocabulary)ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.<br>ë‹¨ì–´ì¥ì„ ë§Œë“¤ ë•ŒëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow ë²„ì „ ì¶œë ¥\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\")\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±. (Tensorflow 2.2.0 ì´í•˜)\n",
    "tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "\n",
    "# (ì£¼ì˜) Tensorflow 2.3.0 ì´ìƒì˜ ë²„ì „ì—ì„œëŠ” ì•„ë˜ ì£¼ì„ì˜ ì½”ë“œë¥¼ ëŒ€ì‹  ì‹¤í–‰í•´ ì£¼ì„¸ìš”. \n",
    "#tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬\n",
    "# ë‹¨ì–´ì¥ì˜ ë²ˆí˜¸ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡ ê°ê° ë‹¨ì–´ì¥ì˜ í¬ê¸°ì™€ ê·¸ë³´ë‹¤ 1ì´ í° ìˆ˜ë¥¼ ë²ˆí˜¸ë¡œ ë¶€ì—¬\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ë¶€ì—¬ëœ ì •ìˆ˜ë¥¼ ì¶œë ¥\n",
    "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°ê° 8,331ê³¼ 8,332ë¼ëŠ” ì ì—ì„œ í˜„ì¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ê°€ 8,331(0ë²ˆë¶€í„° 8,330ë²ˆ)ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **âœ“ ê° ë‹¨ì–´ë¥¼ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ì¸ì½”ë”©(Integer encoding) & íŒ¨ë”©(Padding)**\n",
    "---\n",
    "ìœ„ì—ì„œ **`tensorflow_datasets`**ì˜ **`SubwordTextEncoder`**ë¥¼ ì‚¬ìš©í•´ì„œ tokenizerë¥¼ ì •ì˜í•˜ê³  Vocabularyë¥¼ ë§Œë“¤ì—ˆë‹¤ë©´, **`tokenizer.encode()`**ë¡œ ê° ë‹¨ì–´ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ìˆê³  ë˜ëŠ” **`tokenizer.decode()`**ë¥¼ í†µí•´ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
    "# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê° ë‹¨ì–´ì— ê³ ìœ í•œ ì •ìˆ˜ê°€ ë¶€ì—¬ëœ Vocabularyë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ì‹œí€€ìŠ¤ê°€ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ì¸ì½”ë”©ëœ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ê²°ê³¼ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ ì…‹ì— ëŒ€í•´ì„œ ì „ë¶€ **ì •ìˆ˜ ì¸ì½”ë”©**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ì™€ ë™ì‹œì— ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì •í•˜ê³ , í•´ë‹¹ ê¸¸ì´ë¡œ **íŒ¨ë”©(padding)**í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "        tokenized_inputs.append(sentence1)\n",
    "        tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ì–´ì¥ì˜ í¬ê¸°ì™€ ìƒ˜í”Œì˜ ê°œìˆ˜ë¥¼ í™•ì¸\n",
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)), 'ê°œ')\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)), 'ê°œ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **âœ“ êµì‚¬ ê°•ìš”(Teacher Forcing) ì‚¬ìš©í•˜ê¸°**\n",
    "---\n",
    "ë””ì½”ë”ì˜ ì…ë ¥ê³¼ ì‹¤ì œê°’(ë ˆì´ë¸”)ì„ ì •ì˜í•´ì£¼ê¸° ìœ„í•´ì„œëŠ” **êµì‚¬ ê°•ìš”(Teacher Forcing)**ì´ë¼ëŠ” ì–¸ì–´ ëª¨ë¸ì˜ í›ˆë ¨ ê¸°ë²•ì„ ì´í•´í•´ì•¼ë§Œ í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì„ **tf.data.Dataset API**ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì´ë•Œ, êµì‚¬ ê°•ìš”ë¥¼ ìœ„í•´ì„œ **`answers[:, :-1]`**ë¥¼ ë””ì½”ë”ì˜ ì…ë ¥ê°’, **`answers[:, 1:]`**ë¥¼ ë””ì½”ë”ì˜ ë ˆì´ë¸”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°\n",
    "---\n",
    "ìœ„ ì‹¤ìŠµ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "\t# ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
    "  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # ì¸ì½”ë”\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # ë””ì½”ë”\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ ëª¨ë¸ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì›í™œí•œ í›ˆë ¨ì„ ìœ„í•´ ë…¼ë¬¸ì—ì„œë³´ë‹¤ëŠ” ì‘ì€ ê°’ì„ ì‚¬ìš©\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ ì†ì‹¤ í•¨ìˆ˜(Loss function)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **âœ“ ì»¤ìŠ¤í…€ëœ í•™ìŠµë¥ (Learning rate) : ë§¤ìš° ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°**\n",
    "---\n",
    "- **ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§(Custom Learning rate Scheduling)** : ëª¨ë¸í•™ìŠµ ì´ˆê¸°ì— learning rateë¥¼ ê¸‰ê²©íˆ ë†’ì˜€ë‹¤ê°€, ì´í›„ train stepì´ ì§„í–‰ë¨ì— ë”°ë¼ ì„œì„œíˆ ë‚®ì¶”ì–´ ê°€ë©´ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ í•˜ëŠ” ê³ ê¸‰ ê¸°ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ ê³„íšì„ ì‹œê°í™”\n",
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ ëª¨ë¸ ì»´íŒŒì¼ : ì†ì‹¤ í•¨ìˆ˜ì™€ ì»¤ìŠ¤í…€ ëœ í•™ìŠµë¥ (learning rate)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì»´íŒŒì¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ“ í›ˆë ¨í•˜ê¸° : ì´ 20 ì—í¬í¬ë¥¼ í•™ìŠµ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5. ëª¨ë¸ í‰ê°€í•˜ê¸°\n",
    "---\n",
    "Step 1ì—ì„œ ì„ íƒí•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ëŒ€ë‹µì„ ì–»ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì˜ˆì¸¡(inference) ë‹¨ê³„ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.\n",
    "\n",
    "1. ìƒˆë¡œìš´ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œëŠ” í›ˆë ¨ ë•Œì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œë‹¤.\n",
    "2. ì…ë ¥ ë¬¸ì¥ì„ í† í¬ë‚˜ì´ì§•í•˜ê³ , **`START_TOKEN`**ê³¼ **`END_TOKEN`**ì„ ì¶”ê°€í•œë‹¤.\n",
    "3. íŒ¨ë”© ë§ˆìŠ¤í‚¹ê³¼ ë£© ì–´í—¤ë“œ ë§ˆìŠ¤í‚¹ì„ ê³„ì‚°í•œë‹¤.\n",
    "4. ë””ì½”ë”ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤.\n",
    "5. ë””ì½”ë”ëŠ” ì˜ˆì¸¡ëœ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ê¸°ì¡´ì˜ ì…ë ¥ ì‹œí€€ìŠ¤ì— ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.\n",
    "6. **`END_TOKEN`**ì´ ì˜ˆì¸¡ë˜ê±°ë‚˜ ë¬¸ì¥ì˜ ìµœëŒ€ ê¸¸ì´ì— ë„ë‹¬í•˜ë©´ ë””ì½”ë”ëŠ” ë™ì‘ì„ ë©ˆì¶˜ë‹¤.<br>\n",
    "ìœ„ì˜ ê³¼ì •ì„ ëª¨ë‘ ë‹´ì€ **`decoder_inference()`** í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "    # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "    # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "        # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ì˜ì˜ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ **`decoder_inference()`** í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì±—ë´‡ì˜ ëŒ€ë‹µì„ ì–»ëŠ” **`sentence_generation()`** í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('ì…ë ¥ : {}'.format(sentence))\n",
    "    print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ¼ ì´ì œ ì„ì˜ì˜ ë¬¸ì¥ìœ¼ë¡œë¶€í„° ì±—ë´‡ì˜ ëŒ€ë‹µì„ ì–»ì–´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation('ë‚˜ë‘ ëˆˆì‚¬ëŒ ë§Œë“¤ë˜?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_generation(\"ë‹¤ ìŠì–´~ ë‹¤ ìŠì–´~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë£¨ë¸Œë¦­ í‰ê°€ ê¸°ì¤€\n",
    "---\n",
    "ë²ˆí˜¸|í‰ê°€ë¬¸í•­|ìƒì„¸ê¸°ì¤€\n",
    "---|---|---\n",
    "1 |í•œêµ­ì–´ ì „ì²˜ë¦¬ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì˜€ë‹¤.|ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬, í† í¬ë‚˜ì´ì§•, ë³‘ë ¬ë°ì´í„° êµ¬ì¶•ì˜ ê³¼ì •ì´ ì ì ˆíˆ ì§„í–‰ë˜ì—ˆë‹¤.\n",
    "2 |íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬ í•œêµ­ì–´ ì±—ë´‡ ëª¨ë¸ í•™ìŠµì„ ì •ìƒì ìœ¼ë¡œ ì§„í–‰í•˜ì˜€ë‹¤.|êµ¬í˜„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ í•œêµ­ì–´ ë³‘ë ¬ ë°ì´í„° í•™ìŠµ ì‹œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì˜€ë‹¤.\n",
    "3 |í•œêµ­ì–´ ì…ë ¥ë¬¸ì¥ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì˜€ë‹¤.|í•œêµ­ì–´ ì…ë ¥ë¬¸ì¥ì— ê·¸ëŸ´ë“¯í•œ í•œêµ­ì–´ë¡œ ë‹µë³€ì„ ë¦¬í„´í•˜ì˜€ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# íšŒê³ \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
