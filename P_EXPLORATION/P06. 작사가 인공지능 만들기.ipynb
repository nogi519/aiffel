{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "geological-stereo",
   "metadata": {},
   "source": [
    "# ë©‹ì§„ ì¸ê³µì§€ëŠ¥ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-waters",
   "metadata": {},
   "source": [
    "ì‘ê³¡ì–¸ì–´ëŠ” (ì•„ì‰½ê²Œë„)ì˜ì–´ì…ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ìˆœì„œë¡œ ë§Œë“¤ì–´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-piano",
   "metadata": {},
   "source": [
    "> **Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ  \n",
    "Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°  \n",
    "Step 3. ë°ì´í„° ì •ì œ  \n",
    "Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬  \n",
    "Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-harvey",
   "metadata": {},
   "source": [
    "ê·¸ëŸ¼ ì‹œì‘í•´ë³¼ê¹Œìš”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-knight",
   "metadata": {},
   "source": [
    "### Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-revolution",
   "metadata": {},
   "source": [
    "> ë¨¼ì € ì•„ë˜ ë§í¬ì—ì„œ Song Lyrics ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•´ ì£¼ì„¸ìš”!<br>ì €ì¥ëœ íŒŒì¼ì„ ì••ì¶• í•´ì œí•œ í›„, ëª¨ë“  txt íŒŒì¼ì„ lyrics í´ë”ë¥¼ ë§Œë“¤ì–´ ê·¸ ì†ì— ì €ì¥í•´ì£¼ì„¸ìš”!<br>ì•„ë‹ˆë©´ ê·¸ ì•„ë˜ì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì…”ë„ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-toilet",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/paultimothymooney/poetry/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-optimization",
   "metadata": {},
   "source": [
    "1. wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "2. unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  <span style=\"color:green\">#lyrics í´ë”ì— ì••ì¶•í’€ê¸°</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-wilson",
   "metadata": {},
   "source": [
    "### Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-pepper",
   "metadata": {},
   "source": [
    "glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë©´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì‘ì—…ì„ í•˜ê¸°ê°€ ì•„ì£¼ ìš©ì´í•´ìš”.  \n",
    "glob ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ë„ë¡ í• ê²Œìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medical-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " ['', '', \"Jesus died for somebody's sins but not mine\"]\n"
     ]
    }
   ],
   "source": [
    "import re                  # ì •ê·œí‘œí˜„ì‹ì„ ìœ„í•œ Regex ì§€ì› ëª¨ë“ˆ (ë¬¸ì¥ ë°ì´í„°ë¥¼ ì •ëˆí•˜ê¸° ìœ„í•´) \n",
    "import numpy as np         # ë³€í™˜ëœ ë¬¸ì¥ ë°ì´í„°(í–‰ë ¬)ì„ í¸í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´\n",
    "import tensorflow as tf    # ëŒ€ë§ì˜ í…ì„œí”Œë¡œìš°!\n",
    "import glob                # íŒŒì¼ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë½‘ì„ ë•Œ ì‚¬ìš©\n",
    "import os                  # ìš´ì˜ ì²´ì œì˜ í™˜ê²½ ë³€ìˆ˜ ê°’ì„ ì½ì–´ì˜¬ ìˆ˜ ìˆìŒ!\n",
    "\n",
    "# íŒŒì¼ì„ ì½ê¸°ëª¨ë“œë¡œ ì—´ì–´ ë´…ë‹ˆë‹¤.\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'  # * : all\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []  # ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpusì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-correspondence",
   "metadata": {},
   "source": [
    "### Step 3. ë°ì´í„° ì •ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-information",
   "metadata": {},
   "source": [
    "##### ì´ë²ˆ ìŠ¤í…ì—ì„œ ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê¸° ìœ„í•´ ê±°ì¹  ê³¼ì •ì…ë‹ˆë‹¤. \n",
    "\n",
    ">- ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ corpus ìƒì„±\n",
    ">- tf.keras.preprocessing.text.Tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    ">- tf.data.Dataset.from_tensor_slices()ë¥¼ ì´ìš©í•´ corpus í…ì„œë¥¼ tf.data.Datasetê°ì²´ë¡œ ë³€í™˜\n",
    "\n",
    "datasetì„ ì–»ìœ¼ë©´ ë°ì´í„° ë‹¤ë“¬ê¸° ê³¼ì •ì€ ëë‚©ë‹ˆë‹¤.<br>\n",
    "tf.data.Datasetì—ì„œ ì œê³µí•˜ëŠ” shuffle(), batch() ë“± ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ ê´€ë ¨ ê¸°ëŠ¥ì„ ì†ì‰½ê²Œ ì´ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆêµ°ìš”.\n",
    "\n",
    "ì´ ëª¨ë“  ì¼ë ¨ì˜ ê³¼ì •ì„ í…ì„œí”Œë¡œìš°ì—ì„œì˜ **ë°ì´í„° ì „ì²˜ë¦¬**ë¼ ì¹­í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-operation",
   "metadata": {},
   "source": [
    "**ë¬¸ì¥ ìƒì„±ì— ì í•©í•œ ëª¨ì–‘ìƒˆë¡œ ë°ì´í„°ë¥¼ ì •ì œí•˜ì„¸ìš”!**\n",
    "\n",
    "preprocess_sentence() í•¨ìˆ˜ë¥¼ ë§Œë“  ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ? ì´ë¥¼ í™œìš©í•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì¶”ê°€ë¡œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ê³¼ë„í•œ Paddingì„ ê°–ê²Œ í•˜ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤. ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë…¸ë˜ê°€ì‚¬ ì‘ì‚¬í•˜ê¸°ì— ì–´ìš¸ë¦¬ì§€ ì•Šì„ìˆ˜ë„ ìˆê² ì£ .\n",
    "ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” ë¬¸ì¥ì„ **í† í°í™”** í–ˆì„ ë•Œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì„ í•™ìŠµë°ì´í„°ì—ì„œ ì œì™¸í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-parameter",
   "metadata": {},
   "source": [
    "ê°€ì¥ ì‹¬í”Œí•œ ë°©ë²•ì€ ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²•ì´ì§€ë§Œ ì´ ë°©ë²•ì—ëŠ” ëª‡ê°€ì§€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "##### ëª‡ ê°€ì§€ ë¬¸ì œ ì¼€ì´ìŠ¤\n",
    "1. Hi, my name is John. *(\"Hi,\" \"my\", â€¦, \"john.\" ìœ¼ë¡œ ë¶„ë¦¬ë¨) -ë¬¸ì¥ë¶€í˜¸\n",
    "2. First, open the first chapter. *(Firstì™€ firstë¥¼ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹) -ëŒ€ì†Œë¬¸ì\n",
    "3. He is a ten-year-old boy. *(ten-year-oldë¥¼ í•œ ë‹¨ì–´ë¡œ ì¸ì‹) -íŠ¹ìˆ˜ë¬¸ì\n",
    "\n",
    "\"1.\" ì„ ë§‰ê¸° ìœ„í•´ ë¬¸ì¥ ë¶€í˜¸ ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€ í•  ê±°ê³ ìš”,<br>\n",
    "\"2.\" ë¥¼ ë§‰ê¸° ìœ„í•´ ëª¨ë“  ë¬¸ìë“¤ì„ ì†Œë¬¸ìë¡œ ë³€í™˜í•  ê²ë‹ˆë‹¤.<br>\n",
    "\"3.\" ì„ ë§‰ê¸° ìœ„í•´ íŠ¹ìˆ˜ë¬¸ìë“¤ì€ ëª¨ë‘ ì œê±°í•˜ë„ë¡ í•˜ì£ !\n",
    "\n",
    "ì´ëŸ° ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ì •ê·œí‘œí˜„ì‹(Regex)ì„ ì´ìš©í•œ í•„í„°ë§ì´ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alpha-drive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ì–‘ìª½ ê³µë°±ì„ ì‚­ì œ\n",
    "  \n",
    "    # ì•„ë˜ 3ë‹¨ê³„ë¥¼ ê±°ì³ sentenceëŠ” ìŠ¤í˜ì´ìŠ¤ 1ê°œë¥¼ delimeterë¡œ í•˜ëŠ” ì†Œë¬¸ì ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë°”ë€ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)        # íŒ¨í„´ì˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ë§Œë‚˜ë©´ íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # ê³µë°± íŒ¨í„´ì„ ë§Œë‚˜ë©´ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence)  # a-zA-Z?.!,Â¿ íŒ¨í„´ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ê³µë°±ë¬¸ìê¹Œì§€ë„)ë¥¼ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # ì´ì „ ìŠ¤í…ì—ì„œ ë³¸ ê²ƒì²˜ëŸ¼ ë¬¸ì¥ ì•ë’¤ë¡œ <start>ì™€ <end>ë¥¼ ë‹¨ì–´ì²˜ëŸ¼ ë¶™ì—¬ ì¤ë‹ˆë‹¤\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-weekly",
   "metadata": {},
   "source": [
    "ì§œì”, ì§€ì €ë¶„í•œ ë¬¸ì¥ì„ ë„£ì–´ë„ ì˜ˆì˜ê²Œ ë³€í™˜í•´ì£¼ëŠ” ì •ì œ í•¨ìˆ˜ê°€ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!<br>ë³´ë„ˆìŠ¤ë¡œ start, end ë„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.  \n",
    "\n",
    "ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ëª¨ë¸ì˜ ì…ë ¥ì´ ë˜ëŠ” ë¬¸ì¥ì„ ì†ŒìŠ¤ ë¬¸ì¥(Source Sentence), ì •ë‹µ ì—­í• ì„ í•˜ê²Œ ë  ëª¨ë¸ì˜ ì¶œë ¥ ë¬¸ì¥ì„ íƒ€ê²Ÿ ë¬¸ì¥(Target Sentence)ë¼ê³  ê´€ë¡€ì ìœ¼ë¡œ ë¶€ë¦…ë‹ˆë‹¤. ê°ê° X_train, y_train ì— í•´ë‹¹í•œë‹¤ê³  í•  ìˆ˜ ìˆê² ì£ ?\n",
    "\n",
    "ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ìœ„ì—ì„œ ë§Œë“  ì •ì œ í•¨ìˆ˜ë¥¼ í†µí•´ ë§Œë“  ë°ì´í„°ì…‹ì—ì„œ í† í°í™”ë¥¼ ì§„í–‰í•œ í›„ ë ë‹¨ì–´ <end>ë¥¼ ì—†ì• ë©´ ì†ŒìŠ¤ ë¬¸ì¥, ì²« ë‹¨ì–´ <start>ë¥¼ ì—†ì• ë©´ íƒ€ê²Ÿ ë¬¸ì¥ì´ ë˜ê² ì£ ? ì´ ì •ì œ í•¨ìˆ˜ë¥¼ í™œìš©í•´ì„œ ì•„ë˜ì™€ ê°™ì´ ì •ì œ ë°ì´í„°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tutorial-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or how things used to be And I had to tell Willy every look stand out \n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0 or len(sentence) > 15: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "        \n",
    "corpus[:10]\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-personality",
   "metadata": {},
   "source": [
    "ì´ì œ ë°ì´í„°ëŠ” ì™„ë²½í•˜ê²Œ ì¤€ë¹„ê°€ ëœ ê²ƒ ê°™ë„¤ìš”!<br>\n",
    "\n",
    "í…ì„œí”Œë¡œìš°ëŠ” ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì—¬ëŸ¬ ê°€ì§€ ëª¨ë“ˆì„ ì œê³µí•˜ëŠ”ë°, ìš°ë¦¬ë„ ê·¸ ëª¨ë“ˆì„ ì‹­ë¶„ í™œìš©í•  ê²ë‹ˆë‹¤!<br>\n",
    "ì•„ë˜ì—ì„œ í™œìš©í•˜ê²Œ ë  **tf.keras.preprocessing.text.Tokenizer íŒ¨í‚¤ì§€**ëŠ” ì •ì œëœ ë°ì´í„°ë¥¼ í† í°í™”í•˜ê³ , ë‹¨ì–´ ì‚¬ì „(vocabulary ë˜ëŠ” dictionaryë¼ê³  ì¹­í•¨)ì„ ë§Œë“¤ì–´ì£¼ë©°, ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜ê¹Œì§€ í•œ ë°©ì— í•´ì¤ë‹ˆë‹¤.<br>\n",
    "ì´ ê³¼ì •ì„ ë²¡í„°í™”(vectorize) ë¼ í•˜ë©°, ìˆ«ìë¡œ ë³€í™˜ëœ ë°ì´í„°ë¥¼ í…ì„œ(tensor) ë¼ê³  ì¹­í•©ë‹ˆë‹¤.<br>\n",
    "ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” í…ì„œí”Œë¡œìš°ë¡œ ë§Œë“  ëª¨ë¸ì˜ ì…ì¶œë ¥ ë°ì´í„°ëŠ” ì‹¤ì œë¡œëŠ” ëª¨ë‘ ì´ëŸ° í…ì„œë¡œ ë³€í™˜ë˜ì–´ ì²˜ë¦¬ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coordinate-attendance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   15 1908 ...    0    0    0]\n",
      " [   2   70   56 ...    0    0    0]\n",
      " [   2   70   56 ...    0    0    0]\n",
      " ...\n",
      " [   2 1001 3416 ...    0    0    0]\n",
      " [   2  138  138 ...    0    0    0]\n",
      " [   2  138  138 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7ff0c2883b90>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # í…ì„œí”Œë¡œìš°ì—ì„œ ì œê³µí•˜ëŠ” Tokenizer íŒ¨í‚¤ì§€ë¥¼ ìƒì„±\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000,  # ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜ \n",
    "        filters=' ',    # ë³„ë„ë¡œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, ì‚¬ì „ì— ì—†ì—ˆë˜ ë‹¨ì–´ëŠ” ì–´ë–¤ í† í°ìœ¼ë¡œ ëŒ€ì²´í• ì§€\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # ìš°ë¦¬ê°€ êµ¬ì¶•í•œ corpusë¡œë¶€í„° Tokenizerê°€ ì‚¬ì „ì„ ìë™êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "    # ì´í›„ tokenizerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizerëŠ” êµ¬ì¶•í•œ ì‚¬ì „ìœ¼ë¡œë¶€í„° corpusë¥¼ í•´ì„í•´ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•œ padding  ë©”ì†Œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "    # maxlenì˜ ë””í´íŠ¸ê°’ì€ Noneì…ë‹ˆë‹¤. ì´ ê²½ìš° corpusì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë§ì¶°ì§‘ë‹ˆë‹¤.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-gateway",
   "metadata": {},
   "source": [
    "í…ì„œ ë°ì´í„°ëŠ” ëª¨ë‘ ì •ìˆ˜ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.<br>\n",
    "ì´ ìˆ«ìëŠ” ë‹¤ë¦„ ì•„ë‹ˆë¼, tokenizerì— êµ¬ì¶•ëœ ë‹¨ì–´ ì‚¬ì „ì˜ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.<br>\n",
    "ë‹¨ì–´ ì‚¬ì „ì´ ì–´ë–»ê²Œ êµ¬ì¶•ë˜ì—ˆëŠ”ì§€ ì•„ë˜ì™€ ê°™ì´ í™•ì¸í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominant-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : .\n",
      "7 : you\n",
      "8 : oh\n",
      "9 : it\n",
      "10 : me\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-collaboration",
   "metadata": {},
   "source": [
    "ì–´ë–»ìŠµë‹ˆê¹Œ? 2ë²ˆ ì¸ë±ìŠ¤ê°€ ë°”ë¡œ **start**ì˜€ìŠµë‹ˆë‹¤.<br>\n",
    "ì™œ ëª¨ë“  í–‰ì´ 2ë¡œ ì‹œì‘í•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ì œ ìƒì„±ëœ í…ì„œë¥¼ ì†ŒìŠ¤ì™€ íƒ€ê²Ÿìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ê² ìŠµë‹ˆë‹¤.<br>\n",
    "ì´ ê³¼ì •ë„í…ì„œí”Œë¡œìš°ê°€ ì œê³µí•˜ëŠ” ëª¨ë“ˆì„ ì‚¬ìš©í•  ê²ƒì´ë‹ˆ, ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ë§Œ ëˆˆì—¬ê²¨ ë´ë‘¡ì‹œë‹¤.\n",
    "\n",
    "í…ì„œ ì¶œë ¥ë¶€ì—ì„œ í–‰ ë’¤ìª½ì— 0ì´ ë§ì´ ë‚˜ì˜¨ ë¶€ë¶„ì€ ì •í•´ì§„ ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ë³´ë‹¤ ë¬¸ì¥ì´ ì§§ì„ ê²½ìš° 0ìœ¼ë¡œ íŒ¨ë”©(padding)ì„ ì±„ì›Œë„£ì€ ê²ƒì…ë‹ˆë‹¤.<br>\n",
    "ì‚¬ì „ì—ëŠ” ì—†ì§€ë§Œ 0ì€ ë°”ë¡œ íŒ¨ë”© ë¬¸ì **pad**ê°€ ë  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "current-swaziland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   15 1908   15  522    3    0    0    0    0    0    0]\n",
      "[  15 1908   15  522    3    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    # tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-gentleman",
   "metadata": {},
   "source": [
    "ë§ˆì§€ë§‰ìœ¼ë¡œ ë°ì´í„°ì…‹ ê°ì²´ë¥¼ ìƒì„±í•  ê²ƒì…ë‹ˆë‹¤.<br>\n",
    "í…ì„œí”Œë¡œìš°ë¥¼ í™œìš©í•  ê²½ìš° í…ì„œë¡œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì´ìš©í•´ tf.data.Datasetê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ í”íˆ ì‚¬ìš©í•©ë‹ˆë‹¤.<br>\n",
    "tf.data.Datasetê°ì²´ëŠ” í…ì„œí”Œë¡œìš°ì—ì„œ ì‚¬ìš©í•  ê²½ìš° ë°ì´í„° ì…ë ¥ íŒŒì´í”„ë¼ì¸ì„ í†µí•œ ì†ë„ ê°œì„  ë° ê°ì¢… í¸ì˜ê¸°ëŠ¥ì„ ì œê³µí•˜ë¯€ë¡œ ê¼­ ì‚¬ìš©ë²•ì„ ì•Œì•„ ë‘ëŠ”ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.<br>\n",
    "ì´ë¯¸ ìœ„ì—ì„œ ë°ì´í„°ì…‹ì„ í…ì„œ í˜•íƒœë¡œ ìƒì„±í•´ ë‘ì—ˆìœ¼ë¯€ë¡œ, tf.data.Dataset.from_tensor_slices() ë©”ì†Œë“œë¥¼ ì´ìš©í•´ tf.data.Datasetê°ì²´ë¥¼ ìƒì„±í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "returning-syndicate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 12), (256, 12)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 7000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-interface",
   "metadata": {},
   "source": [
    "### Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-computer",
   "metadata": {},
   "source": [
    "**í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì„¸ìš”!**\n",
    "\n",
    "tokenize() í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•œ í›„, sklearn ëª¨ë“ˆì˜ train_test_split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.  \n",
    "ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”!  \n",
    "\n",
    "ì´ ë°ì´í„°ì˜ **20%** ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incoming-productivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (11296, 12)\n",
      "Target Train: (11296, 12)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-account",
   "metadata": {},
   "source": [
    "ë§Œì•½ í•™ìŠµë°ì´í„° ê°¯ìˆ˜ê°€ 124960ë³´ë‹¤ í¬ë‹¤ë©´ ìœ„ Step 3.ì˜ ë°ì´í„° ì •ì œ ê³¼ì •ì„ ë‹¤ì‹œí•œë²ˆ ê²€í† í•´ ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-commons",
   "metadata": {},
   "source": [
    "### Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-navigation",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì˜ **Embedding Size**ì™€ **Hidden Size**ë¥¼ ì¡°ì ˆí•˜ë©° **10 Epoch** ì•ˆì— **val_loss ê°’ì„ 2.2 ìˆ˜ì¤€**ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ì„¸ìš”!  \n",
    "(LossëŠ” ì•„ë˜ ì œì‹œëœ Loss í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "elect-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss í•¨ìˆ˜\n",
    "\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#     from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-mandate",
   "metadata": {},
   "source": [
    "\"\"\"\"\"\", '''''' ë¡œ ì¤„ ì£¼ì„ì²˜ë¦¬ í•˜ë ¤ë‹ˆ ì•ˆë˜ë„¤ìš”..  \n",
    "ì£¼ì„ì„ í•˜ê³  ì‹¶ì€ ë¶€ë¶„ë“¤ì„ ë“œë˜ê·¸ í•´ì„œ ì˜ì—­ì„ ì„ íƒí•œ í›„ Ctrl + / ë¥¼ ëˆ„ë¥´ë©´ ëœë‹¤ê¸¸ë˜ í•´ë³´ë‹ˆ #ìœ¼ë¡œ ì¤„ë§ˆë‹¤ ì£¼ì„ì²˜ë¦¬ ë©ë‹ˆë‹¤.ğŸ¤”ğŸ¤”ğŸ¤”\n",
    "ì´ ë¶€ë¶„ì€ ì¶”í›„ ë‹¤ì‹œ ì•Œì•„ë³´ê¸°ë¡œ.....ğŸ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "recorded-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prospective-fraud",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 12, 7001), dtype=float32, numpy=\n",
       "array([[[-1.5630612e-04, -2.1479298e-04,  4.7894078e-04, ...,\n",
       "         -5.5313401e-04, -5.6747791e-05,  1.4602030e-05],\n",
       "        [-2.0139404e-04, -6.6264608e-04,  1.8932828e-04, ...,\n",
       "         -1.1565241e-03,  1.5133073e-05, -1.6830349e-04],\n",
       "        [-2.5849909e-04, -7.7325472e-04,  1.2038055e-05, ...,\n",
       "         -1.0125804e-03,  1.8405824e-04, -5.3904357e-04],\n",
       "        ...,\n",
       "        [ 3.5242317e-03,  1.7246478e-03,  8.5587282e-04, ...,\n",
       "         -1.9012154e-03,  2.7056127e-05,  9.5006684e-04],\n",
       "        [ 4.1646911e-03,  2.1634460e-03,  1.1990423e-03, ...,\n",
       "         -2.0568324e-03,  5.3903928e-05,  1.1703369e-03],\n",
       "        [ 4.7409656e-03,  2.5637052e-03,  1.5439735e-03, ...,\n",
       "         -2.1806376e-03,  5.6141813e-05,  1.3372115e-03]],\n",
       "\n",
       "       [[-1.5630612e-04, -2.1479298e-04,  4.7894078e-04, ...,\n",
       "         -5.5313401e-04, -5.6747791e-05,  1.4602030e-05],\n",
       "        [-5.1237922e-04, -3.9897300e-04,  6.8201660e-04, ...,\n",
       "         -7.6508749e-04, -2.5922109e-04,  2.9711300e-04],\n",
       "        [-5.5576651e-04, -4.4431939e-04,  6.1630859e-04, ...,\n",
       "         -1.0284091e-03, -7.7318202e-04,  2.3160210e-04],\n",
       "        ...,\n",
       "        [ 3.8977903e-03,  2.5076531e-03,  1.8976004e-03, ...,\n",
       "         -2.4826997e-03, -5.1990146e-04,  1.7902314e-03],\n",
       "        [ 4.5214631e-03,  2.8635836e-03,  2.1723078e-03, ...,\n",
       "         -2.5667308e-03, -4.4663067e-04,  1.8525427e-03],\n",
       "        [ 5.0698342e-03,  3.1797988e-03,  2.4401976e-03, ...,\n",
       "         -2.6152076e-03, -3.9783449e-04,  1.8850341e-03]],\n",
       "\n",
       "       [[-1.5630612e-04, -2.1479298e-04,  4.7894078e-04, ...,\n",
       "         -5.5313401e-04, -5.6747791e-05,  1.4602030e-05],\n",
       "        [-5.0401967e-04, -5.8407249e-04,  6.9987384e-04, ...,\n",
       "         -5.9602427e-04, -4.9588874e-05, -3.0222620e-04],\n",
       "        [-4.9512793e-04, -7.0657616e-04,  6.1148801e-04, ...,\n",
       "         -7.6515088e-04, -4.1928704e-04, -6.3239102e-04],\n",
       "        ...,\n",
       "        [ 4.0416368e-03,  2.3594850e-03,  1.7830939e-03, ...,\n",
       "         -2.2971486e-03, -2.1747241e-04,  1.2966642e-03],\n",
       "        [ 4.6440475e-03,  2.7389824e-03,  2.0568459e-03, ...,\n",
       "         -2.4063704e-03, -1.9520604e-04,  1.4329929e-03],\n",
       "        [ 5.1711970e-03,  3.0765580e-03,  2.3254280e-03, ...,\n",
       "         -2.4762845e-03, -1.9448251e-04,  1.5296759e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.5630612e-04, -2.1479298e-04,  4.7894078e-04, ...,\n",
       "         -5.5313401e-04, -5.6747791e-05,  1.4602030e-05],\n",
       "        [-4.2037331e-04, -2.5561903e-04,  9.6807058e-04, ...,\n",
       "         -5.9313618e-04, -2.2778916e-04,  6.5407294e-05],\n",
       "        [-4.6677279e-04,  3.4137222e-05,  1.2325167e-03, ...,\n",
       "         -3.2277557e-04, -3.2806088e-04,  2.3083738e-04],\n",
       "        ...,\n",
       "        [ 1.7932968e-03,  1.3039856e-03,  1.0206837e-03, ...,\n",
       "         -1.0094164e-03, -8.9595490e-04,  1.6443052e-03],\n",
       "        [ 2.4684181e-03,  1.7558883e-03,  1.1874133e-03, ...,\n",
       "         -1.3859567e-03, -7.8200310e-04,  1.8182923e-03],\n",
       "        [ 3.1268043e-03,  2.1750012e-03,  1.4068638e-03, ...,\n",
       "         -1.7169002e-03, -6.8043330e-04,  1.9301210e-03]],\n",
       "\n",
       "       [[-1.5630612e-04, -2.1479298e-04,  4.7894078e-04, ...,\n",
       "         -5.5313401e-04, -5.6747791e-05,  1.4602030e-05],\n",
       "        [-3.6445481e-04, -3.3522650e-04,  4.1821817e-04, ...,\n",
       "         -6.7537889e-04,  1.0400027e-04, -2.3185520e-04],\n",
       "        [-6.1062595e-04, -4.9791340e-04,  3.5953865e-04, ...,\n",
       "         -4.1074978e-04,  2.2533124e-04, -2.9200179e-04],\n",
       "        ...,\n",
       "        [ 2.8253859e-03,  1.3828625e-03,  1.2464977e-03, ...,\n",
       "         -1.8990134e-03, -3.0817455e-04,  4.2864069e-04],\n",
       "        [ 3.5642099e-03,  1.8087784e-03,  1.5421839e-03, ...,\n",
       "         -2.0846613e-03, -2.8185284e-04,  6.8430434e-04],\n",
       "        [ 4.2376844e-03,  2.1973234e-03,  1.8460384e-03, ...,\n",
       "         -2.2306596e-03, -2.6551550e-04,  8.9156721e-04]],\n",
       "\n",
       "       [[-1.5630612e-04, -2.1479298e-04,  4.7894078e-04, ...,\n",
       "         -5.5313401e-04, -5.6747791e-05,  1.4602030e-05],\n",
       "        [-9.4741721e-05, -3.7796432e-04,  8.7885297e-04, ...,\n",
       "         -9.9733821e-04,  7.4214411e-05, -2.9498275e-04],\n",
       "        [ 9.4426839e-05, -4.7334284e-04,  9.8026975e-04, ...,\n",
       "         -9.3696319e-04,  2.3618696e-04, -4.3183300e-04],\n",
       "        ...,\n",
       "        [ 3.8431222e-03,  1.8227332e-03,  1.6863194e-03, ...,\n",
       "         -2.3394884e-03, -7.4542790e-05,  9.7196922e-04],\n",
       "        [ 4.4117696e-03,  2.2093586e-03,  1.9479269e-03, ...,\n",
       "         -2.4411189e-03, -3.9046703e-05,  1.1688846e-03],\n",
       "        [ 4.9253902e-03,  2.5653408e-03,  2.2119468e-03, ...,\n",
       "         -2.5059809e-03, -2.4163030e-05,  1.3193979e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-rouge",
   "metadata": {},
   "source": [
    "ResourceExhaustedError: OOM when allocating tensor with shape[88576,7001] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul] ì—ëŸ¬ê°€ ë°œëª©ì„ ì¡ëŠ”ë‹¹.....ã… ã… ã… ã… "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-resolution",
   "metadata": {},
   "source": [
    "modelì˜ input shapeê°€ ê²°ì •ë˜ë©´ì„œ model.build()ê°€ ìë™ìœ¼ë¡œ í˜¸ì¶œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pretty-charity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-mirror",
   "metadata": {},
   "source": [
    "ì´ì œ ëª¨ë¸ì´ í•™ìŠµí•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.<br>\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œë³´ì„¸ìš”!<br>\n",
    "í•™ìŠµì—” 10ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤(GPU í™˜ê²½ ê¸°ì¤€)<br>\n",
    "í˜¹ì‹œë¼ë„ í•™ìŠµì— ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤ë©´ tf.test.is_gpu_available() ì†ŒìŠ¤ë¥¼ ì‹¤í–‰í•´ í…ì„œí”Œë¡œìš°ê°€ GPUë¥¼ ì˜ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "canadian-pendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 5s 90ms/step - loss: 0.8971\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 5s 90ms/step - loss: 0.8626\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 5s 86ms/step - loss: 0.8490\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 0.8393\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 5s 86ms/step - loss: 0.8288\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 5s 85ms/step - loss: 0.8208\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 5s 84ms/step - loss: 0.8130\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 5s 89ms/step - loss: 0.8061\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 0.7990\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 5s 88ms/step - loss: 0.7933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fefa9e69150>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "devoted-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  \n",
    " \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # ë¬¸ì¥ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spectacular-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ ì¼ë‹¨ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í• ë•ŒëŠ” ë£¨í”„ë¥¼ ëŒë©´ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë°”ë¡œ ìƒˆë¡­ê²Œ ìƒì„±í•œ ë‹¨ì–´ê°€ ë©ë‹ˆë‹¤. \n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ ìƒˆë¡­ê²Œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ ë¬¸ì¥ì˜ ë’¤ì— ë¶™ì—¬ ì¤ë‹ˆë‹¤. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´  while ë£¨í”„ë¥¼ ë˜ ëŒë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # ìƒì„±ëœ tensor ì•ˆì— ìˆëŠ” word indexë¥¼ tokenizer.index_word ì‚¬ì „ì„ í†µí•´ ì‹¤ì œ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-flower",
   "metadata": {},
   "source": [
    "ëª¨ë¸ì´ ìƒì„±í•œ ê°€ì‚¬ í•œ ì¤„ì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "checked-traffic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-survival",
   "metadata": {},
   "source": [
    "ë¼ ê·¸ëŸ´ë“¯í•˜ì£ ? ã…ã…ã…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-appendix",
   "metadata": {},
   "source": [
    "## ë£¨ë¸Œë¦­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-labor",
   "metadata": {},
   "source": [
    "1. ê°€ì‚¬ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ê°€?\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´ì…˜ ê²°ê³¼ê°€ ê·¸ëŸ´ë“¯í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±ë˜ëŠ”ê°€? ë„µ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-function",
   "metadata": {},
   "source": [
    "2. ë°ì´í„°ì˜ ì „ì²˜ë¦¬ì™€ ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ê°€?\n",
    "\n",
    "íŠ¹ìˆ˜ë¬¸ì ì œê±°, í† í¬ë‚˜ì´ì € ìƒì„±, íŒ¨ë”©ì²˜ë¦¬ ë“±ì˜ ê³¼ì •ì´ ë¹ ì§ì—†ì´ ì§„í–‰ë˜ì—ˆëŠ”ê°€? ë„µ!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-might",
   "metadata": {},
   "source": [
    "3. í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆëŠ”ê°€?\n",
    "\n",
    "í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì˜ validation lossê°€ 2.2 ì´í•˜ë¡œ ë‚®ì•„ì¡ŒëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-museum",
   "metadata": {},
   "source": [
    "## íšŒê³ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-clear",
   "metadata": {},
   "source": [
    "ë¹¡ì„¼ ì €ë²ˆì£¼ì˜ ë…¸ë“œë³´ë‹¤ í›¨ì”¬ ê°„ê²°í•´ì„œ ì´ê²Œ ë§ë‚˜...? í–ˆëŠ”ë° ì¡°ì›ë“¤ê³¼ ì–˜ê¸°í•´ë³´ë‹ˆ ì €ë²ˆì£¼ ë…¸ë“œê³¼ì œë“¤ì´ ë‚œë„ê°€ ë†’ì•˜ë‹¤ëŠ” ê±¸ ìƒˆì‚¼ ëŠê¼ˆë‹¤.  \n",
    "ì•„ë¬´ë˜ë„ ğŸ‡°ğŸ‡·í•œêµ­ì–´ë³´ë‹¤ ê³µê°œëœ ğŸ‡ºğŸ‡¸ì˜ì–´ë°ì´í„°ê°€ ë§ì•„ ì•„ì‰½ê¸´í•˜ì§€ë§Œ ì´ë²ˆì£¼ ë…¸ë“œëŠ” ì¢€ ë” ì¬ë°Œê²Œ í•  ìˆ˜ ìˆì—ˆë‹¤.  \n",
    "ë‹¤ë§Œ ì˜ì–´ê³µë¶€ë„ ë‹¤ì‹œ í•´ì•¼...í•  í•„ìš”ì„±ì„ ê³„ì† ëŠë¼ê³  ìˆì–´ì„œ ê³„íšì„ ì„¸ì›Œë´ì•¼ê² ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
